<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>microgpt.py (Karpathy) — the code in plain English, line by line | Art & Articles by Emin Mahrt</title><meta name=keywords content="Karpathy,GPT,Python,Machine Learning"><meta name=description content="A mom-readable walkthrough of Andrej Karpathy’s dependency-free GPT training script — every line translated to plain English."><meta name=author content="Emin Henri Mahrt"><link rel=canonical href=https://emino.app/posts/microgpt-karpathy-line-by-line/><link crossorigin=anonymous href=/assets/css/stylesheet.4372053d75d77ffd536f4c15275103ea7a53accf0bb252b18bfacc814611d64b.css integrity="sha256-Q3IFPXXXf/1Tb0wVJ1ED6npTrM8LslKxi/rMgUYR1ks=" rel="preload stylesheet" as=style><link rel=icon href=https://emino.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://emino.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://emino.app/favicon-32x32.png><link rel=apple-touch-icon href=https://emino.app/apple-touch-icon.png><link rel=mask-icon href=https://emino.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://emino.app/posts/microgpt-karpathy-line-by-line/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://emino.app/posts/microgpt-karpathy-line-by-line/"><meta property="og:site_name" content="Art & Articles by Emin Mahrt"><meta property="og:title" content="microgpt.py (Karpathy) — the code in plain English, line by line"><meta property="og:description" content="A mom-readable walkthrough of Andrej Karpathy’s dependency-free GPT training script — every line translated to plain English."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-15T21:45:00+00:00"><meta property="article:modified_time" content="2026-02-15T21:45:00+00:00"><meta property="article:tag" content="Karpathy"><meta property="article:tag" content="GPT"><meta property="article:tag" content="Python"><meta property="article:tag" content="Machine Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="microgpt.py (Karpathy) — the code in plain English, line by line"><meta name=twitter:description content="A mom-readable walkthrough of Andrej Karpathy’s dependency-free GPT training script — every line translated to plain English."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://emino.app/posts/"},{"@type":"ListItem","position":2,"name":"microgpt.py (Karpathy) — the code in plain English, line by line","item":"https://emino.app/posts/microgpt-karpathy-line-by-line/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"microgpt.py (Karpathy) — the code in plain English, line by line","name":"microgpt.py (Karpathy) — the code in plain English, line by line","description":"A mom-readable walkthrough of Andrej Karpathy’s dependency-free GPT training script — every line translated to plain English.","keywords":["Karpathy","GPT","Python","Machine Learning"],"articleBody":"\nThis post contains two things:\nThe original microgpt.py code (as-is) A plain-English explanation for every line, in the same order, so a non-programmer can still follow the story. 1) The original code (verbatim) \"\"\" The most atomic way to train and inference a GPT in pure, dependency-free Python. This file is the complete algorithm. Everything else is just efficiency. @karpathy \"\"\" import os # os.path.exists import math # math.log, math.exp import random # random.seed, random.choices, random.gauss, random.shuffle random.seed(42) # Let there be order among chaos # Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names) if not os.path.exists('input.txt'): import urllib.request names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt' urllib.request.urlretrieve(names_url, 'input.txt') docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents random.shuffle(docs) print(f\"num docs: {len(docs)}\") # Let there be a Tokenizer to translate strings to discrete symbols and back uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1 BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS print(f\"vocab size: {vocab_size}\") # Let there be Autograd, to recursively apply the chain rule through a computation graph class Value: __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage def __init__(self, data, children=(), local_grads=()): self.data = data # scalar value of this node calculated during forward pass self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass self._children = children # children of this node in the computation graph self._local_grads = local_grads # local derivative of this node w.r.t. its children def __add__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data + other.data, (self, other), (1, 1)) def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data * other.data, (self, other), (other.data, self.data)) def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),)) def log(self): return Value(math.log(self.data), (self,), (1/self.data,)) def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),)) def relu(self): return Value(max(0, self.data), (self,), (float(self.data \u003e 0),)) def __neg__(self): return self * -1 def __radd__(self, other): return self + other def __sub__(self, other): return self + (-other) def __rsub__(self, other): return other + (-self) def __rmul__(self, other): return self * other def __truediv__(self, other): return self * other**-1 def __rtruediv__(self, other): return other * self**-1 def backward(self): topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._children: build_topo(child) topo.append(v) build_topo(self) self.grad = 1 for v in reversed(topo): for child, local_grad in zip(v._children, v._local_grads): child.grad += local_grad * v.grad # Initialize the parameters, to store the knowledge of the model. n_embd = 16 # embedding dimension n_head = 4 # number of attention heads n_layer = 1 # number of layers block_size = 16 # maximum sequence length head_dim = n_embd // n_head # dimension of each head matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)] state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)} for i in range(n_layer): state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd) state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd) params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value] print(f\"num params: {len(params)}\") # Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next. # Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -\u003e rmsnorm, no biases, GeLU -\u003e ReLU def linear(x, w): return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w] def softmax(logits): max_val = max(val.data for val in logits) exps = [(val - max_val).exp() for val in logits] total = sum(exps) return [e / total for e in exps] def rmsnorm(x): ms = sum(xi * xi for xi in x) / len(x) scale = (ms + 1e-5) ** -0.5 return [xi * scale for xi in x] def gpt(token_id, pos_id, keys, values): tok_emb = state_dict['wte'][token_id] # token embedding pos_emb = state_dict['wpe'][pos_id] # position embedding x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding x = rmsnorm(x) for li in range(n_layer): # 1) Multi-head attention block x_residual = x x = rmsnorm(x) q = linear(x, state_dict[f'layer{li}.attn_wq']) k = linear(x, state_dict[f'layer{li}.attn_wk']) v = linear(x, state_dict[f'layer{li}.attn_wv']) keys[li].append(k) values[li].append(v) x_attn = [] for h in range(n_head): hs = h * head_dim q_h = q[hs:hs+head_dim] k_h = [ki[hs:hs+head_dim] for ki in keys[li]] v_h = [vi[hs:hs+head_dim] for vi in values[li]] attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))] attn_weights = softmax(attn_logits) head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)] x_attn.extend(head_out) x = linear(x_attn, state_dict[f'layer{li}.attn_wo']) x = [a + b for a, b in zip(x, x_residual)] # 2) MLP block x_residual = x x = rmsnorm(x) x = linear(x, state_dict[f'layer{li}.mlp_fc1']) x = [xi.relu() for xi in x] x = linear(x, state_dict[f'layer{li}.mlp_fc2']) x = [a + b for a, b in zip(x, x_residual)] logits = linear(x, state_dict['lm_head']) return logits # Let there be Adam, the blessed optimizer and its buffers learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8 m = [0.0] * len(params) # first moment buffer v = [0.0] * len(params) # second moment buffer # Repeat in sequence num_steps = 1000 # number of training steps for step in range(num_steps): # Take single document, tokenize it, surround it with BOS special token on both sides doc = docs[step % len(docs)] tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS] n = min(block_size, len(tokens) - 1) # Forward the token sequence through the model, building up the computation graph all the way to the loss. keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] losses = [] for pos_id in range(n): token_id, target_id = tokens[pos_id], tokens[pos_id + 1] logits = gpt(token_id, pos_id, keys, values) probs = softmax(logits) loss_t = -probs[target_id].log() losses.append(loss_t) loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low. # Backward the loss, calculating the gradients with respect to all model parameters. loss.backward() # Adam optimizer update: update the model parameters based on the corresponding gradients. lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay for i, p in enumerate(params): m[i] = beta1 * m[i] + (1 - beta1) * p.grad v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2 m_hat = m[i] / (1 - beta1 ** (step + 1)) v_hat = v[i] / (1 - beta2 ** (step + 1)) p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam) p.grad = 0 print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\") # Inference: may the model babble back to us temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high print(\"\\n--- inference (new, hallucinated names) ---\") for sample_idx in range(20): keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] token_id = BOS sample = [] for pos_id in range(block_size): logits = gpt(token_id, pos_id, keys, values) probs = softmax([l / temperature for l in logits]) token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0] if token_id == BOS: break sample.append(uchars[token_id]) print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\") 2) A story-driven walkthrough (so you can reimplement it) This section explains the script like a narrated build. If you can read Python and basic algebra, you should be able to re-create the program from scratch after going through this.\nI’ll keep two promises:\nI’ll explain what each part is doing and why it exists. I’ll connect the pieces so you can see the full flow: data → tokens → model → loss → gradients → optimizer → sampling. Step 0: The goal (in one sentence) We want a model that reads a sequence of tokens (characters) and predicts the next token. Training teaches it to assign high probability to the correct next character. After training, we can sample characters one by one to generate new “name-like” strings.\nStep 1: Get a tiny dataset The script expects a text file input.txt with one training example per line. If it’s missing, it downloads a classic dataset: a list of names.\nWhy shuffle?\nShuffling makes training less biased by file ordering.\nStep 2: Build a character tokenizer The tokenizer here is intentionally minimal:\nCollect every unique character that appears in the dataset. Assign each character an integer id. Add one special token called BOS (“beginning of sequence”). So the vocabulary is:\n0..len(uchars)-1 → actual characters BOS = len(uchars) → special marker Why BOS?\nIt lets us:\nstart generation from a single known token also mark the end of a generated name (the script uses BOS as both “start” and “stop”) Step 3: A tiny autograd engine (the Value class) Everything in the model is built out of scalar nodes called Value.\nA Value stores:\ndata: the number (forward pass) grad: d(loss)/d(this) (backward pass) references to its children + the local derivatives needed for chain rule When you do math like a*b + c, the code builds a computation graph automatically.\nBackprop, conceptually build a topological ordering of nodes (children before parents) set loss.grad = 1 walk the nodes in reverse topo order and propagate gradients to children This is the same idea as micrograd, but implemented directly here.\nStep 4: Initialize model parameters (“weights”) The script defines a tiny GPT with a few hyperparameters:\nn_embd: embedding size (vector length per token) n_head: attention heads n_layer: transformer layers block_size: maximum context length Then it creates a state_dict with matrices (lists of lists of Value) for:\nToken embeddings wte[vocab_size][n_embd] Position embeddings wpe[block_size][n_embd] Per-layer attention weights (Wq, Wk, Wv, Wo) Per-layer MLP weights (fc1, fc2) Language-model head lm_head[vocab_size][n_embd] (projects hidden state → logits) All weights start as small random numbers.\nImportant:\nThis implementation is fully “manual” (no NumPy). It’s slow, but it’s the algorithm in its simplest form.\nStep 5: Define the core math building blocks 5.1 Linear layer linear(x, w) computes a matrix multiply:\ninput x is a vector length nin weights w is a matrix [nout][nin] output is a vector length nout 5.2 Softmax Softmax turns logits into probabilities:\nsubtract max(logit) for numerical stability exponentiate divide by sum So probabilities sum to 1.\n5.3 RMSNorm RMSNorm rescales a vector so its average squared magnitude is ~1:\ncompute mean square: ms = mean(x_i^2) compute scale: scale = 1/sqrt(ms + eps) return x * scale This stabilizes training.\nStep 6: The GPT forward pass (one position at a time) The gpt(token_id, pos_id, keys, values) function produces logits for the next token.\n6.1 Embeddings lookup token embedding: tok_emb = wte[token_id] lookup position embedding: pos_emb = wpe[pos_id] add them: x = tok_emb + pos_emb normalize: x = rmsnorm(x) 6.2 Transformer blocks (repeat for each layer) Each layer has two sub-blocks:\nA) Multi-head self-attention\ncreate q, k, v by applying linear layers to x append k, v into the running keys[layer] / values[layer] cache for each head: slice out head dimensions compute attention scores: dot(q, k_t) / sqrt(d) softmax scores → weights weighted sum of v’s → head output concatenate all head outputs apply output projection Wo add residual connection B) MLP\nfc1 → ReLU → fc2 add residual connection So the shape is always: vector length n_embd.\n6.3 Final projection to vocab logits = lm_head * x gives one logit per vocab token.\nThose logits are turned into probabilities by softmax.\nStep 7: Training loop (how it learns) Each training step uses one document (name).\n7.1 Tokenize one document The code creates a token list like:\n[BOS] + [char ids for doc] + [BOS]\nThen it takes up to block_size transitions.\nSo if tokens are t0, t1, t2, ..., we train on pairs:\ninput t0 → target t1 input t1 → target t2 … 7.2 Forward pass to compute loss For each position:\nrun gpt(token_id, pos_id, keys, values) softmax → probs take the probability assigned to the correct target_id negative log-likelihood loss: -log(probs[target_id]) Average across positions → final loss.\n7.3 Backward pass loss.backward() walks the graph and fills gradients for every parameter Value.\n7.4 Adam update Adam keeps two moving averages per parameter:\nm: first moment (mean gradient) v: second moment (mean squared gradient) Then it applies bias correction (m_hat, v_hat) and updates:\np.data -= lr * m_hat / (sqrt(v_hat) + eps)\nIt also uses a linearly decaying learning rate.\nStep 8: Inference (how it generates names) After training, we sample names like this:\nstart with token_id = BOS for each position up to block_size: run GPT forward divide logits by temperature softmax → probabilities sample next token id using those probabilities if token == BOS: stop (end-of-name) else append the corresponding character This produces 20 hallucinated names.\nIf you want to reimplement it (minimal pseudocode) Here’s the skeleton you could rewrite from scratch:\nload dataset lines build vocab + BOS implement Value + backward() init weights in state_dict implement linear, softmax, rmsnorm implement gpt() (embeddings → blocks → logits) training loop: tokenize → compute NLL loss → backprop → Adam update sampling loop: autoregressive generation ","wordCount":"2199","inLanguage":"en","datePublished":"2026-02-15T21:45:00Z","dateModified":"2026-02-15T21:45:00Z","author":{"@type":"Person","name":"Emin Henri Mahrt"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://emino.app/posts/microgpt-karpathy-line-by-line/"},"publisher":{"@type":"Organization","name":"Art \u0026 Articles by Emin Mahrt","logo":{"@type":"ImageObject","url":"https://emino.app/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://emino.app/ accesskey=h title="Art & Articles by Emin Mahrt (Alt + H)">Art & Articles by Emin Mahrt</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://emino.app/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://emino.app/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://emino.app/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://emino.app/>Home</a>&nbsp;»&nbsp;<a href=https://emino.app/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">microgpt.py (Karpathy) — the code in plain English, line by line</h1><div class=post-description>A mom-readable walkthrough of Andrej Karpathy’s dependency-free GPT training script — every line translated to plain English.</div><div class=post-meta><span title='2026-02-15 21:45:00 +0000 UTC'>February 15, 2026</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Emin Henri Mahrt</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-the-original-code-verbatim aria-label="1) The original code (verbatim)">1) The original code (verbatim)</a></li><li><a href=#2-a-story-driven-walkthrough-so-you-can-reimplement-it aria-label="2) A story-driven walkthrough (so you can reimplement it)">2) A story-driven walkthrough (so you can reimplement it)</a><ul><li><a href=#step-0-the-goal-in-one-sentence aria-label="Step 0: The goal (in one sentence)">Step 0: The goal (in one sentence)</a></li><li><a href=#step-1-get-a-tiny-dataset aria-label="Step 1: Get a tiny dataset">Step 1: Get a tiny dataset</a></li><li><a href=#step-2-build-a-character-tokenizer aria-label="Step 2: Build a character tokenizer">Step 2: Build a character tokenizer</a></li><li><a href=#step-3-a-tiny-autograd-engine-the-value-class aria-label="Step 3: A tiny autograd engine (the Value class)">Step 3: A tiny autograd engine (the Value class)</a><ul><li><a href=#backprop-conceptually aria-label="Backprop, conceptually">Backprop, conceptually</a></li></ul></li><li><a href=#step-4-initialize-model-parameters-weights aria-label="Step 4: Initialize model parameters (“weights”)">Step 4: Initialize model parameters (“weights”)</a></li><li><a href=#step-5-define-the-core-math-building-blocks aria-label="Step 5: Define the core math building blocks">Step 5: Define the core math building blocks</a><ul><li><a href=#51-linear-layer aria-label="5.1 Linear layer">5.1 Linear layer</a></li><li><a href=#52-softmax aria-label="5.2 Softmax">5.2 Softmax</a></li><li><a href=#53-rmsnorm aria-label="5.3 RMSNorm">5.3 RMSNorm</a></li></ul></li><li><a href=#step-6-the-gpt-forward-pass-one-position-at-a-time aria-label="Step 6: The GPT forward pass (one position at a time)">Step 6: The GPT forward pass (one position at a time)</a><ul><li><a href=#61-embeddings aria-label="6.1 Embeddings">6.1 Embeddings</a></li><li><a href=#62-transformer-blocks-repeat-for-each-layer aria-label="6.2 Transformer blocks (repeat for each layer)">6.2 Transformer blocks (repeat for each layer)</a></li><li><a href=#63-final-projection-to-vocab aria-label="6.3 Final projection to vocab">6.3 Final projection to vocab</a></li></ul></li><li><a href=#step-7-training-loop-how-it-learns aria-label="Step 7: Training loop (how it learns)">Step 7: Training loop (how it learns)</a><ul><li><a href=#71-tokenize-one-document aria-label="7.1 Tokenize one document">7.1 Tokenize one document</a></li><li><a href=#72-forward-pass-to-compute-loss aria-label="7.2 Forward pass to compute loss">7.2 Forward pass to compute loss</a></li><li><a href=#73-backward-pass aria-label="7.3 Backward pass">7.3 Backward pass</a></li><li><a href=#74-adam-update aria-label="7.4 Adam update">7.4 Adam update</a></li></ul></li><li><a href=#step-8-inference-how-it-generates-names aria-label="Step 8: Inference (how it generates names)">Step 8: Inference (how it generates names)</a></li></ul></li><li><a href=#if-you-want-to-reimplement-it-minimal-pseudocode aria-label="If you want to reimplement it (minimal pseudocode)">If you want to reimplement it (minimal pseudocode)</a></li></ul></div></details></div><div class=post-content><p><img loading=lazy src=/media/microgpt-karpathy-line-by-line/cover.jpg></p><p>This post contains <strong>two things</strong>:</p><ol><li>The original <code>microgpt.py</code> code (as-is)</li><li>A <strong>plain-English explanation for every line</strong>, in the same order, so a non-programmer can still follow the story.</li></ol><hr><h2 id=1-the-original-code-verbatim>1) The original code (verbatim)<a hidden class=anchor aria-hidden=true href=#1-the-original-code-verbatim>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>The most atomic way to train and inference a GPT in pure, dependency-free Python.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>This file is the complete algorithm.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Everything else is just efficiency.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@karpathy
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os       <span style=color:#75715e># os.path.exists</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math     <span style=color:#75715e># math.log, math.exp</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random   <span style=color:#75715e># random.seed, random.choices, random.gauss, random.shuffle</span>
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>) <span style=color:#75715e># Let there be order among chaos</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#39;input.txt&#39;</span>):
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> urllib.request
</span></span><span style=display:flex><span>    names_url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt&#39;</span>
</span></span><span style=display:flex><span>    urllib<span style=color:#f92672>.</span>request<span style=color:#f92672>.</span>urlretrieve(names_url, <span style=color:#e6db74>&#39;input.txt&#39;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> [l<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> open(<span style=color:#e6db74>&#39;input.txt&#39;</span>)<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>) <span style=color:#66d9ef>if</span> l<span style=color:#f92672>.</span>strip()] <span style=color:#75715e># list[str] of documents</span>
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>shuffle(docs)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;num docs: </span><span style=color:#e6db74>{</span>len(docs)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be a Tokenizer to translate strings to discrete symbols and back</span>
</span></span><span style=display:flex><span>uchars <span style=color:#f92672>=</span> sorted(set(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(docs))) <span style=color:#75715e># unique characters in the dataset become token ids 0..n-1</span>
</span></span><span style=display:flex><span>BOS <span style=color:#f92672>=</span> len(uchars) <span style=color:#75715e># token id for the special Beginning of Sequence (BOS) token</span>
</span></span><span style=display:flex><span>vocab_size <span style=color:#f92672>=</span> len(uchars) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span> <span style=color:#75715e># total number of unique tokens, +1 is for BOS</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;vocab size: </span><span style=color:#e6db74>{</span>vocab_size<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be Autograd, to recursively apply the chain rule through a computation graph</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Value</span>:
</span></span><span style=display:flex><span>    __slots__ <span style=color:#f92672>=</span> (<span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;grad&#39;</span>, <span style=color:#e6db74>&#39;_children&#39;</span>, <span style=color:#e6db74>&#39;_local_grads&#39;</span>) <span style=color:#75715e># Python optimization for memory usage</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, data, children<span style=color:#f92672>=</span>(), local_grads<span style=color:#f92672>=</span>()):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> data                <span style=color:#75715e># scalar value of this node calculated during forward pass</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>                   <span style=color:#75715e># derivative of the loss w.r.t. this node, calculated in backward pass</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_children <span style=color:#f92672>=</span> children       <span style=color:#75715e># children of this node in the computation graph</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_local_grads <span style=color:#f92672>=</span> local_grads <span style=color:#75715e># local derivative of this node w.r.t. its children</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__add__</span>(self, other):
</span></span><span style=display:flex><span>        other <span style=color:#f92672>=</span> other <span style=color:#66d9ef>if</span> isinstance(other, Value) <span style=color:#66d9ef>else</span> Value(other)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Value(self<span style=color:#f92672>.</span>data <span style=color:#f92672>+</span> other<span style=color:#f92672>.</span>data, (self, other), (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__mul__</span>(self, other):
</span></span><span style=display:flex><span>        other <span style=color:#f92672>=</span> other <span style=color:#66d9ef>if</span> isinstance(other, Value) <span style=color:#66d9ef>else</span> Value(other)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Value(self<span style=color:#f92672>.</span>data <span style=color:#f92672>*</span> other<span style=color:#f92672>.</span>data, (self, other), (other<span style=color:#f92672>.</span>data, self<span style=color:#f92672>.</span>data))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__pow__</span>(self, other): <span style=color:#66d9ef>return</span> Value(self<span style=color:#f92672>.</span>data<span style=color:#f92672>**</span>other, (self,), (other <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>data<span style=color:#f92672>**</span>(other<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>),))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>log</span>(self): <span style=color:#66d9ef>return</span> Value(math<span style=color:#f92672>.</span>log(self<span style=color:#f92672>.</span>data), (self,), (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>self<span style=color:#f92672>.</span>data,))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>exp</span>(self): <span style=color:#66d9ef>return</span> Value(math<span style=color:#f92672>.</span>exp(self<span style=color:#f92672>.</span>data), (self,), (math<span style=color:#f92672>.</span>exp(self<span style=color:#f92672>.</span>data),))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu</span>(self): <span style=color:#66d9ef>return</span> Value(max(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>data), (self,), (float(self<span style=color:#f92672>.</span>data <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>),))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__neg__</span>(self): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__radd__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>+</span> other
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__sub__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>+</span> (<span style=color:#f92672>-</span>other)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__rsub__</span>(self, other): <span style=color:#66d9ef>return</span> other <span style=color:#f92672>+</span> (<span style=color:#f92672>-</span>self)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__rmul__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> other
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__truediv__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> other<span style=color:#f92672>**-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__rtruediv__</span>(self, other): <span style=color:#66d9ef>return</span> other <span style=color:#f92672>*</span> self<span style=color:#f92672>**-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(self):
</span></span><span style=display:flex><span>        topo <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        visited <span style=color:#f92672>=</span> set()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_topo</span>(v):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> v <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                visited<span style=color:#f92672>.</span>add(v)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> child <span style=color:#f92672>in</span> v<span style=color:#f92672>.</span>_children:
</span></span><span style=display:flex><span>                    build_topo(child)
</span></span><span style=display:flex><span>                topo<span style=color:#f92672>.</span>append(v)
</span></span><span style=display:flex><span>        build_topo(self)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> v <span style=color:#f92672>in</span> reversed(topo):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> child, local_grad <span style=color:#f92672>in</span> zip(v<span style=color:#f92672>.</span>_children, v<span style=color:#f92672>.</span>_local_grads):
</span></span><span style=display:flex><span>                child<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> local_grad <span style=color:#f92672>*</span> v<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize the parameters, to store the knowledge of the model.</span>
</span></span><span style=display:flex><span>n_embd <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>     <span style=color:#75715e># embedding dimension</span>
</span></span><span style=display:flex><span>n_head <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>      <span style=color:#75715e># number of attention heads</span>
</span></span><span style=display:flex><span>n_layer <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>     <span style=color:#75715e># number of layers</span>
</span></span><span style=display:flex><span>block_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span> <span style=color:#75715e># maximum sequence length</span>
</span></span><span style=display:flex><span>head_dim <span style=color:#f92672>=</span> n_embd <span style=color:#f92672>//</span> n_head <span style=color:#75715e># dimension of each head</span>
</span></span><span style=display:flex><span>matrix <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> nout, nin, std<span style=color:#f92672>=</span><span style=color:#ae81ff>0.08</span>: [[Value(random<span style=color:#f92672>.</span>gauss(<span style=color:#ae81ff>0</span>, std)) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nin)] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nout)]
</span></span><span style=display:flex><span>state_dict <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;wte&#39;</span>: matrix(vocab_size, n_embd), <span style=color:#e6db74>&#39;wpe&#39;</span>: matrix(block_size, n_embd), <span style=color:#e6db74>&#39;lm_head&#39;</span>: matrix(vocab_size, n_embd)}
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_layer):
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wq&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wk&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wv&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wo&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc1&#39;</span>] <span style=color:#f92672>=</span> matrix(<span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc2&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> n_embd)
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> [p <span style=color:#66d9ef>for</span> mat <span style=color:#f92672>in</span> state_dict<span style=color:#f92672>.</span>values() <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> mat <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> row] <span style=color:#75715e># flatten params into a single list[Value]</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;num params: </span><span style=color:#e6db74>{</span>len(params)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -&gt; rmsnorm, no biases, GeLU -&gt; ReLU</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linear</span>(x, w):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [sum(wi <span style=color:#f92672>*</span> xi <span style=color:#66d9ef>for</span> wi, xi <span style=color:#f92672>in</span> zip(wo, x)) <span style=color:#66d9ef>for</span> wo <span style=color:#f92672>in</span> w]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(logits):
</span></span><span style=display:flex><span>    max_val <span style=color:#f92672>=</span> max(val<span style=color:#f92672>.</span>data <span style=color:#66d9ef>for</span> val <span style=color:#f92672>in</span> logits)
</span></span><span style=display:flex><span>    exps <span style=color:#f92672>=</span> [(val <span style=color:#f92672>-</span> max_val)<span style=color:#f92672>.</span>exp() <span style=color:#66d9ef>for</span> val <span style=color:#f92672>in</span> logits]
</span></span><span style=display:flex><span>    total <span style=color:#f92672>=</span> sum(exps)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [e <span style=color:#f92672>/</span> total <span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> exps]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>rmsnorm</span>(x):
</span></span><span style=display:flex><span>    ms <span style=color:#f92672>=</span> sum(xi <span style=color:#f92672>*</span> xi <span style=color:#66d9ef>for</span> xi <span style=color:#f92672>in</span> x) <span style=color:#f92672>/</span> len(x)
</span></span><span style=display:flex><span>    scale <span style=color:#f92672>=</span> (ms <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-5</span>) <span style=color:#f92672>**</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [xi <span style=color:#f92672>*</span> scale <span style=color:#66d9ef>for</span> xi <span style=color:#f92672>in</span> x]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gpt</span>(token_id, pos_id, keys, values):
</span></span><span style=display:flex><span>    tok_emb <span style=color:#f92672>=</span> state_dict[<span style=color:#e6db74>&#39;wte&#39;</span>][token_id] <span style=color:#75715e># token embedding</span>
</span></span><span style=display:flex><span>    pos_emb <span style=color:#f92672>=</span> state_dict[<span style=color:#e6db74>&#39;wpe&#39;</span>][pos_id] <span style=color:#75715e># position embedding</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> [t <span style=color:#f92672>+</span> p <span style=color:#66d9ef>for</span> t, p <span style=color:#f92672>in</span> zip(tok_emb, pos_emb)] <span style=color:#75715e># joint token and position embedding</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> rmsnorm(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> li <span style=color:#f92672>in</span> range(n_layer):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1) Multi-head attention block</span>
</span></span><span style=display:flex><span>        x_residual <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> rmsnorm(x)
</span></span><span style=display:flex><span>        q <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wq&#39;</span>])
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wk&#39;</span>])
</span></span><span style=display:flex><span>        v <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wv&#39;</span>])
</span></span><span style=display:flex><span>        keys[li]<span style=color:#f92672>.</span>append(k)
</span></span><span style=display:flex><span>        values[li]<span style=color:#f92672>.</span>append(v)
</span></span><span style=display:flex><span>        x_attn <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> range(n_head):
</span></span><span style=display:flex><span>            hs <span style=color:#f92672>=</span> h <span style=color:#f92672>*</span> head_dim
</span></span><span style=display:flex><span>            q_h <span style=color:#f92672>=</span> q[hs:hs<span style=color:#f92672>+</span>head_dim]
</span></span><span style=display:flex><span>            k_h <span style=color:#f92672>=</span> [ki[hs:hs<span style=color:#f92672>+</span>head_dim] <span style=color:#66d9ef>for</span> ki <span style=color:#f92672>in</span> keys[li]]
</span></span><span style=display:flex><span>            v_h <span style=color:#f92672>=</span> [vi[hs:hs<span style=color:#f92672>+</span>head_dim] <span style=color:#66d9ef>for</span> vi <span style=color:#f92672>in</span> values[li]]
</span></span><span style=display:flex><span>            attn_logits <span style=color:#f92672>=</span> [sum(q_h[j] <span style=color:#f92672>*</span> k_h[t][j] <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(head_dim)) <span style=color:#f92672>/</span> head_dim<span style=color:#f92672>**</span><span style=color:#ae81ff>0.5</span> <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(len(k_h))]
</span></span><span style=display:flex><span>            attn_weights <span style=color:#f92672>=</span> softmax(attn_logits)
</span></span><span style=display:flex><span>            head_out <span style=color:#f92672>=</span> [sum(attn_weights[t] <span style=color:#f92672>*</span> v_h[t][j] <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(len(v_h))) <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(head_dim)]
</span></span><span style=display:flex><span>            x_attn<span style=color:#f92672>.</span>extend(head_out)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> linear(x_attn, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wo&#39;</span>])
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [a <span style=color:#f92672>+</span> b <span style=color:#66d9ef>for</span> a, b <span style=color:#f92672>in</span> zip(x, x_residual)]
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2) MLP block</span>
</span></span><span style=display:flex><span>        x_residual <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> rmsnorm(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc1&#39;</span>])
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [xi<span style=color:#f92672>.</span>relu() <span style=color:#66d9ef>for</span> xi <span style=color:#f92672>in</span> x]
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc2&#39;</span>])
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [a <span style=color:#f92672>+</span> b <span style=color:#66d9ef>for</span> a, b <span style=color:#f92672>in</span> zip(x, x_residual)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>&#39;lm_head&#39;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> logits
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be Adam, the blessed optimizer and its buffers</span>
</span></span><span style=display:flex><span>learning_rate, beta1, beta2, eps_adam <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>, <span style=color:#ae81ff>0.85</span>, <span style=color:#ae81ff>0.99</span>, <span style=color:#ae81ff>1e-8</span>
</span></span><span style=display:flex><span>m <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0.0</span>] <span style=color:#f92672>*</span> len(params) <span style=color:#75715e># first moment buffer</span>
</span></span><span style=display:flex><span>v <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0.0</span>] <span style=color:#f92672>*</span> len(params) <span style=color:#75715e># second moment buffer</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Repeat in sequence</span>
</span></span><span style=display:flex><span>num_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span> <span style=color:#75715e># number of training steps</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(num_steps):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Take single document, tokenize it, surround it with BOS special token on both sides</span>
</span></span><span style=display:flex><span>    doc <span style=color:#f92672>=</span> docs[step <span style=color:#f92672>%</span> len(docs)]
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> [BOS] <span style=color:#f92672>+</span> [uchars<span style=color:#f92672>.</span>index(ch) <span style=color:#66d9ef>for</span> ch <span style=color:#f92672>in</span> doc] <span style=color:#f92672>+</span> [BOS]
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> min(block_size, len(tokens) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Forward the token sequence through the model, building up the computation graph all the way to the loss.</span>
</span></span><span style=display:flex><span>    keys, values <span style=color:#f92672>=</span> [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)], [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)]
</span></span><span style=display:flex><span>    losses <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> pos_id <span style=color:#f92672>in</span> range(n):
</span></span><span style=display:flex><span>        token_id, target_id <span style=color:#f92672>=</span> tokens[pos_id], tokens[pos_id <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> gpt(token_id, pos_id, keys, values)
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>        loss_t <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[target_id]<span style=color:#f92672>.</span>log()
</span></span><span style=display:flex><span>        losses<span style=color:#f92672>.</span>append(loss_t)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> n) <span style=color:#f92672>*</span> sum(losses) <span style=color:#75715e># final average loss over the document sequence. May yours be low.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Backward the loss, calculating the gradients with respect to all model parameters.</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Adam optimizer update: update the model parameters based on the corresponding gradients.</span>
</span></span><span style=display:flex><span>    lr_t <span style=color:#f92672>=</span> learning_rate <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> step <span style=color:#f92672>/</span> num_steps) <span style=color:#75715e># linear learning rate decay</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, p <span style=color:#f92672>in</span> enumerate(params):
</span></span><span style=display:flex><span>        m[i] <span style=color:#f92672>=</span> beta1 <span style=color:#f92672>*</span> m[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta1) <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>        v[i] <span style=color:#f92672>=</span> beta2 <span style=color:#f92672>*</span> v[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta2) <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        m_hat <span style=color:#f92672>=</span> m[i] <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta1 <span style=color:#f92672>**</span> (step <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        v_hat <span style=color:#f92672>=</span> v[i] <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta2 <span style=color:#f92672>**</span> (step <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>-=</span> lr_t <span style=color:#f92672>*</span> m_hat <span style=color:#f92672>/</span> (v_hat <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>+</span> eps_adam)
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;step </span><span style=color:#e6db74>{</span>step<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>:</span><span style=color:#e6db74>4d</span><span style=color:#e6db74>}</span><span style=color:#e6db74> / </span><span style=color:#e6db74>{</span>num_steps<span style=color:#e6db74>:</span><span style=color:#e6db74>4d</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | loss </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>data<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Inference: may the model babble back to us</span>
</span></span><span style=display:flex><span>temperature <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span> <span style=color:#75715e># in (0, 1], control the &#34;creativity&#34; of generated text, low to high</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>--- inference (new, hallucinated names) ---&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> sample_idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>    keys, values <span style=color:#f92672>=</span> [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)], [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)]
</span></span><span style=display:flex><span>    token_id <span style=color:#f92672>=</span> BOS
</span></span><span style=display:flex><span>    sample <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> pos_id <span style=color:#f92672>in</span> range(block_size):
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> gpt(token_id, pos_id, keys, values)
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> softmax([l <span style=color:#f92672>/</span> temperature <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> logits])
</span></span><span style=display:flex><span>        token_id <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>choices(range(vocab_size), weights<span style=color:#f92672>=</span>[p<span style=color:#f92672>.</span>data <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> probs])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> token_id <span style=color:#f92672>==</span> BOS:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        sample<span style=color:#f92672>.</span>append(uchars[token_id])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;sample </span><span style=color:#e6db74>{</span>sample_idx<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>:</span><span style=color:#e6db74>2d</span><span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span><span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(sample)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><hr><h2 id=2-a-story-driven-walkthrough-so-you-can-reimplement-it>2) A story-driven walkthrough (so you can reimplement it)<a hidden class=anchor aria-hidden=true href=#2-a-story-driven-walkthrough-so-you-can-reimplement-it>#</a></h2><p>This section explains the script like a narrated build. If you can read Python and basic algebra, you should be able to <strong>re-create the program from scratch</strong> after going through this.</p><p>I’ll keep two promises:</p><ul><li>I’ll explain <em>what each part is doing</em> and <em>why it exists</em>.</li><li>I’ll connect the pieces so you can see the full flow: <strong>data → tokens → model → loss → gradients → optimizer → sampling</strong>.</li></ul><hr><h3 id=step-0-the-goal-in-one-sentence>Step 0: The goal (in one sentence)<a hidden class=anchor aria-hidden=true href=#step-0-the-goal-in-one-sentence>#</a></h3><p>We want a model that reads a sequence of tokens (characters) and predicts the <strong>next token</strong>. Training teaches it to assign high probability to the correct next character. After training, we can sample characters one by one to generate new “name-like” strings.</p><hr><h3 id=step-1-get-a-tiny-dataset>Step 1: Get a tiny dataset<a hidden class=anchor aria-hidden=true href=#step-1-get-a-tiny-dataset>#</a></h3><p>The script expects a text file <code>input.txt</code> with one training example per line. If it’s missing, it downloads a classic dataset: a list of names.</p><p><strong>Why shuffle?</strong></p><p>Shuffling makes training less biased by file ordering.</p><hr><h3 id=step-2-build-a-character-tokenizer>Step 2: Build a character tokenizer<a hidden class=anchor aria-hidden=true href=#step-2-build-a-character-tokenizer>#</a></h3><p>The tokenizer here is intentionally minimal:</p><ul><li>Collect every unique character that appears in the dataset.</li><li>Assign each character an integer id.</li><li>Add one special token called <strong>BOS</strong> (“beginning of sequence”).</li></ul><p>So the vocabulary is:</p><ul><li><code>0..len(uchars)-1</code> → actual characters</li><li><code>BOS = len(uchars)</code> → special marker</li></ul><p><strong>Why BOS?</strong></p><p>It lets us:</p><ul><li>start generation from a single known token</li><li>also mark the end of a generated name (the script uses BOS as both “start” and “stop”)</li></ul><hr><h3 id=step-3-a-tiny-autograd-engine-the-value-class>Step 3: A tiny autograd engine (the <code>Value</code> class)<a hidden class=anchor aria-hidden=true href=#step-3-a-tiny-autograd-engine-the-value-class>#</a></h3><p>Everything in the model is built out of scalar nodes called <code>Value</code>.</p><p>A <code>Value</code> stores:</p><ul><li><code>data</code>: the number (forward pass)</li><li><code>grad</code>: d(loss)/d(this) (backward pass)</li><li>references to its children + the local derivatives needed for chain rule</li></ul><p>When you do math like <code>a*b + c</code>, the code builds a computation graph automatically.</p><h4 id=backprop-conceptually>Backprop, conceptually<a hidden class=anchor aria-hidden=true href=#backprop-conceptually>#</a></h4><ol><li>build a topological ordering of nodes (children before parents)</li><li>set <code>loss.grad = 1</code></li><li>walk the nodes in reverse topo order and propagate gradients to children</li></ol><p>This is the same idea as micrograd, but implemented directly here.</p><hr><h3 id=step-4-initialize-model-parameters-weights>Step 4: Initialize model parameters (“weights”)<a hidden class=anchor aria-hidden=true href=#step-4-initialize-model-parameters-weights>#</a></h3><p>The script defines a <strong>tiny GPT</strong> with a few hyperparameters:</p><ul><li><code>n_embd</code>: embedding size (vector length per token)</li><li><code>n_head</code>: attention heads</li><li><code>n_layer</code>: transformer layers</li><li><code>block_size</code>: maximum context length</li></ul><p>Then it creates a <code>state_dict</code> with matrices (lists of lists of <code>Value</code>) for:</p><ol><li><strong>Token embeddings</strong> <code>wte[vocab_size][n_embd]</code></li><li><strong>Position embeddings</strong> <code>wpe[block_size][n_embd]</code></li><li><strong>Per-layer attention weights</strong> (Wq, Wk, Wv, Wo)</li><li><strong>Per-layer MLP weights</strong> (fc1, fc2)</li><li><strong>Language-model head</strong> <code>lm_head[vocab_size][n_embd]</code> (projects hidden state → logits)</li></ol><p>All weights start as small random numbers.</p><p><strong>Important:</strong></p><p>This implementation is fully “manual” (no NumPy). It’s slow, but it’s the <em>algorithm</em> in its simplest form.</p><hr><h3 id=step-5-define-the-core-math-building-blocks>Step 5: Define the core math building blocks<a hidden class=anchor aria-hidden=true href=#step-5-define-the-core-math-building-blocks>#</a></h3><h4 id=51-linear-layer>5.1 Linear layer<a hidden class=anchor aria-hidden=true href=#51-linear-layer>#</a></h4><p><code>linear(x, w)</code> computes a matrix multiply:</p><ul><li>input <code>x</code> is a vector length <code>nin</code></li><li>weights <code>w</code> is a matrix <code>[nout][nin]</code></li><li>output is a vector length <code>nout</code></li></ul><h4 id=52-softmax>5.2 Softmax<a hidden class=anchor aria-hidden=true href=#52-softmax>#</a></h4><p>Softmax turns logits into probabilities:</p><ul><li>subtract max(logit) for numerical stability</li><li>exponentiate</li><li>divide by sum</li></ul><p>So probabilities sum to 1.</p><h4 id=53-rmsnorm>5.3 RMSNorm<a hidden class=anchor aria-hidden=true href=#53-rmsnorm>#</a></h4><p>RMSNorm rescales a vector so its average squared magnitude is ~1:</p><ul><li>compute mean square: <code>ms = mean(x_i^2)</code></li><li>compute scale: <code>scale = 1/sqrt(ms + eps)</code></li><li>return <code>x * scale</code></li></ul><p>This stabilizes training.</p><hr><h3 id=step-6-the-gpt-forward-pass-one-position-at-a-time>Step 6: The GPT forward pass (one position at a time)<a hidden class=anchor aria-hidden=true href=#step-6-the-gpt-forward-pass-one-position-at-a-time>#</a></h3><p>The <code>gpt(token_id, pos_id, keys, values)</code> function produces logits for the <em>next token</em>.</p><h4 id=61-embeddings>6.1 Embeddings<a hidden class=anchor aria-hidden=true href=#61-embeddings>#</a></h4><ol><li>lookup token embedding: <code>tok_emb = wte[token_id]</code></li><li>lookup position embedding: <code>pos_emb = wpe[pos_id]</code></li><li>add them: <code>x = tok_emb + pos_emb</code></li><li>normalize: <code>x = rmsnorm(x)</code></li></ol><h4 id=62-transformer-blocks-repeat-for-each-layer>6.2 Transformer blocks (repeat for each layer)<a hidden class=anchor aria-hidden=true href=#62-transformer-blocks-repeat-for-each-layer>#</a></h4><p>Each layer has two sub-blocks:</p><p><strong>A) Multi-head self-attention</strong></p><ul><li>create <code>q, k, v</code> by applying linear layers to <code>x</code></li><li>append <code>k, v</code> into the running <code>keys[layer]</code> / <code>values[layer]</code> cache</li><li>for each head:<ul><li>slice out head dimensions</li><li>compute attention scores: dot(q, k_t) / sqrt(d)</li><li>softmax scores → weights</li><li>weighted sum of v’s → head output</li></ul></li><li>concatenate all head outputs</li><li>apply output projection <code>Wo</code></li><li>add residual connection</li></ul><p><strong>B) MLP</strong></p><ul><li>fc1 → ReLU → fc2</li><li>add residual connection</li></ul><p>So the shape is always: vector length <code>n_embd</code>.</p><h4 id=63-final-projection-to-vocab>6.3 Final projection to vocab<a hidden class=anchor aria-hidden=true href=#63-final-projection-to-vocab>#</a></h4><p><code>logits = lm_head * x</code> gives one logit per vocab token.</p><p>Those logits are turned into probabilities by <code>softmax</code>.</p><hr><h3 id=step-7-training-loop-how-it-learns>Step 7: Training loop (how it learns)<a hidden class=anchor aria-hidden=true href=#step-7-training-loop-how-it-learns>#</a></h3><p>Each training step uses one document (name).</p><h4 id=71-tokenize-one-document>7.1 Tokenize one document<a hidden class=anchor aria-hidden=true href=#71-tokenize-one-document>#</a></h4><p>The code creates a token list like:</p><p><code>[BOS] + [char ids for doc] + [BOS]</code></p><p>Then it takes up to <code>block_size</code> transitions.</p><p>So if tokens are <code>t0, t1, t2, ...</code>, we train on pairs:</p><ul><li>input <code>t0</code> → target <code>t1</code></li><li>input <code>t1</code> → target <code>t2</code></li><li>&mldr;</li></ul><h4 id=72-forward-pass-to-compute-loss>7.2 Forward pass to compute loss<a hidden class=anchor aria-hidden=true href=#72-forward-pass-to-compute-loss>#</a></h4><p>For each position:</p><ol><li>run <code>gpt(token_id, pos_id, keys, values)</code></li><li>softmax → <code>probs</code></li><li>take the probability assigned to the correct <code>target_id</code></li><li>negative log-likelihood loss: <code>-log(probs[target_id])</code></li></ol><p>Average across positions → final <code>loss</code>.</p><h4 id=73-backward-pass>7.3 Backward pass<a hidden class=anchor aria-hidden=true href=#73-backward-pass>#</a></h4><p><code>loss.backward()</code> walks the graph and fills gradients for every parameter <code>Value</code>.</p><h4 id=74-adam-update>7.4 Adam update<a hidden class=anchor aria-hidden=true href=#74-adam-update>#</a></h4><p>Adam keeps two moving averages per parameter:</p><ul><li><code>m</code>: first moment (mean gradient)</li><li><code>v</code>: second moment (mean squared gradient)</li></ul><p>Then it applies bias correction (<code>m_hat</code>, <code>v_hat</code>) and updates:</p><p><code>p.data -= lr * m_hat / (sqrt(v_hat) + eps)</code></p><p>It also uses a <strong>linearly decaying learning rate</strong>.</p><hr><h3 id=step-8-inference-how-it-generates-names>Step 8: Inference (how it generates names)<a hidden class=anchor aria-hidden=true href=#step-8-inference-how-it-generates-names>#</a></h3><p>After training, we sample names like this:</p><ol><li>start with <code>token_id = BOS</code></li><li>for each position up to <code>block_size</code>:<ul><li>run GPT forward</li><li>divide logits by <code>temperature</code></li><li>softmax → probabilities</li><li>sample next token id using those probabilities</li><li>if token == BOS: stop (end-of-name)</li><li>else append the corresponding character</li></ul></li></ol><p>This produces 20 hallucinated names.</p><hr><h2 id=if-you-want-to-reimplement-it-minimal-pseudocode>If you want to reimplement it (minimal pseudocode)<a hidden class=anchor aria-hidden=true href=#if-you-want-to-reimplement-it-minimal-pseudocode>#</a></h2><p>Here’s the skeleton you could rewrite from scratch:</p><ol><li>load dataset lines</li><li>build vocab + BOS</li><li>implement <code>Value</code> + <code>backward()</code></li><li>init weights in <code>state_dict</code></li><li>implement <code>linear</code>, <code>softmax</code>, <code>rmsnorm</code></li><li>implement <code>gpt()</code> (embeddings → blocks → logits)</li><li>training loop: tokenize → compute NLL loss → backprop → Adam update</li><li>sampling loop: autoregressive generation</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://emino.app/tags/karpathy/>Karpathy</a></li><li><a href=https://emino.app/tags/gpt/>GPT</a></li><li><a href=https://emino.app/tags/python/>Python</a></li><li><a href=https://emino.app/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://emino.app/posts/microgpt-line-by-line-plain-english/><span class=title>« Prev</span><br><span>microgpt.py explained in plain English (line by line)</span>
</a><a class=next href=https://emino.app/posts/from-starry-sketches-to-superzoom-pixels-galileo-vs-nikon-p1000/><span class=title>Next »</span><br><span>From Starry Sketches to Superzoom Pixels: Galileo's Telescope vs. the Nikon Coolpix P1000</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on x" href="https://x.com/intent/tweet/?text=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line&amp;url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f&amp;hashtags=Karpathy%2cGPT%2cPython%2cMachineLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f&amp;title=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line&amp;summary=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line&amp;source=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on reddit" href="https://reddit.com/submit?url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f&title=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on whatsapp" href="https://api.whatsapp.com/send?text=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line%20-%20https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on telegram" href="https://telegram.me/share/url?text=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line&amp;url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py (Karpathy) — the code in plain English, line by line on ycombinator" href="https://news.ycombinator.com/submitlink?t=microgpt.py%20%28Karpathy%29%20%e2%80%94%20the%20code%20in%20plain%20English%2c%20line%20by%20line&u=https%3a%2f%2femino.app%2fposts%2fmicrogpt-karpathy-line-by-line%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://emino.app/>Art & Articles by Emin Mahrt</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>