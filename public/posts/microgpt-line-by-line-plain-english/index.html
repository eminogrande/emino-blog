<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>microgpt.py explained in plain English (line by line) | Art & Articles by Emin Mahrt</title><meta name=keywords content="GPT,Karpathy,Python,Explained"><meta name=description content="microgpt.py explained in plain English (line by line)
This post rewrites the code line by line into plain English. The original code is by Andrej Karpathy (gist link below).

Original gist: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95


The code
&#34;&#34;&#34;
The most atomic way to train and inference a GPT in pure, dependency-free Python.
This file is the complete algorithm.
Everything else is just efficiency.

@karpathy
&#34;&#34;&#34;

import os       # os.path.exists
import math     # math.log, math.exp
import random   # random.seed, random.choices, random.gauss, random.shuffle
random.seed(42) # Let there be order among chaos

# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # list[str] of documents
random.shuffle(docs)
print(f&#34;num docs: {len(docs)}&#34;)

# Let there be a Tokenizer to translate strings to discrete symbols and back
uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1
BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token
vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS
print(f&#34;vocab size: {vocab_size}&#34;)

# Let there be Autograd, to recursively apply the chain rule through a computation graph
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage

    def __init__(self, data, children=(), local_grads=()):
        self.data = data                # scalar value of this node calculated during forward pass
        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass
        self._children = children       # children of this node in the computation graph
        self._local_grads = local_grads # local derivative of this node w.r.t. its children

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad

# Initialize the parameters, to store the knowledge of the model.
n_embd = 16     # embedding dimension
n_head = 4      # number of attention heads
n_layer = 1     # number of layers
block_size = 16 # maximum sequence length
head_dim = n_embd // n_head # dimension of each head
matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]
state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}
for i in range(n_layer):
    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)
params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]
print(f&#34;num params: {len(params)}&#34;)

# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.
# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU
def linear(x, w):
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]

def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]

def rmsnorm(x):
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]

def gpt(token_id, pos_id, keys, values):
    tok_emb = state_dict['wte'][token_id] # token embedding
    pos_emb = state_dict['wpe'][pos_id] # position embedding
    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding
    x = rmsnorm(x)

    for li in range(n_layer):
        # 1) Multi-head attention block
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f'layer{li}.attn_wq'])
        k = linear(x, state_dict[f'layer{li}.attn_wk'])
        v = linear(x, state_dict[f'layer{li}.attn_wv'])
        keys[li].append(k)
        values[li].append(v)
        x_attn = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]
            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]
            v_h = [vi[hs:hs+head_dim] for vi in values[li]]
            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
            attn_weights = softmax(attn_logits)
            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
            x_attn.extend(head_out)
        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])
        x = [a + b for a, b in zip(x, x_residual)]
        # 2) MLP block
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])
        x = [xi.relu() for xi in x]
        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])
        x = [a + b for a, b in zip(x, x_residual)]

    logits = linear(x, state_dict['lm_head'])
    return logits

# Let there be Adam, the blessed optimizer and its buffers
learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8
m = [0.0] * len(params) # first moment buffer
v = [0.0] * len(params) # second moment buffer

# Repeat in sequence
num_steps = 1000 # number of training steps
for step in range(num_steps):

    # Take single document, tokenize it, surround it with BOS special token on both sides
    doc = docs[step % len(docs)]
    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
    n = min(block_size, len(tokens) - 1)

    # Forward the token sequence through the model, building up the computation graph all the way to the loss.
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    losses = []
    for pos_id in range(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax(logits)
        loss_t = -probs[target_id].log()
        losses.append(loss_t)
    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.

    # Backward the loss, calculating the gradients with respect to all model parameters.
    loss.backward()

    # Adam optimizer update: update the model parameters based on the corresponding gradients.
    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay
    for i, p in enumerate(params):
        m[i] = beta1 * m[i] + (1 - beta1) * p.grad
        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2
        m_hat = m[i] / (1 - beta1 ** (step + 1))
        v_hat = v[i] / (1 - beta2 ** (step + 1))
        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)
        p.grad = 0

    print(f&#34;step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}&#34;)

# Inference: may the model babble back to us
temperature = 0.5 # in (0, 1], control the &#34;creativity&#34; of generated text, low to high
print(&#34;\n--- inference (new, hallucinated names) ---&#34;)
for sample_idx in range(20):
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    token_id = BOS
    sample = []
    for pos_id in range(block_size):
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax([l / temperature for l in logits])
        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]
        if token_id == BOS:
            break
        sample.append(uchars[token_id])
    print(f&#34;sample {sample_idx+1:2d}: {''.join(sample)}&#34;)

Line-by-line plain English
Line 1: &#34;&#34;&#34;"><meta name=author content="Emin Henri Mahrt"><link rel=canonical href=https://emino.app/posts/microgpt-line-by-line-plain-english/><link crossorigin=anonymous href=/assets/css/stylesheet.4372053d75d77ffd536f4c15275103ea7a53accf0bb252b18bfacc814611d64b.css integrity="sha256-Q3IFPXXXf/1Tb0wVJ1ED6npTrM8LslKxi/rMgUYR1ks=" rel="preload stylesheet" as=style><link rel=icon href=https://emino.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://emino.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://emino.app/favicon-32x32.png><link rel=apple-touch-icon href=https://emino.app/apple-touch-icon.png><link rel=mask-icon href=https://emino.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://emino.app/posts/microgpt-line-by-line-plain-english/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://emino.app/posts/microgpt-line-by-line-plain-english/"><meta property="og:site_name" content="Art & Articles by Emin Mahrt"><meta property="og:title" content="microgpt.py explained in plain English (line by line)"><meta property="og:description" content="microgpt.py explained in plain English (line by line) This post rewrites the code line by line into plain English. The original code is by Andrej Karpathy (gist link below).
Original gist: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95 The code &#34;&#34;&#34; The most atomic way to train and inference a GPT in pure, dependency-free Python. This file is the complete algorithm. Everything else is just efficiency. @karpathy &#34;&#34;&#34; import os # os.path.exists import math # math.log, math.exp import random # random.seed, random.choices, random.gauss, random.shuffle random.seed(42) # Let there be order among chaos # Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names) if not os.path.exists('input.txt'): import urllib.request names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt' urllib.request.urlretrieve(names_url, 'input.txt') docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # list[str] of documents random.shuffle(docs) print(f&#34;num docs: {len(docs)}&#34;) # Let there be a Tokenizer to translate strings to discrete symbols and back uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1 BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS print(f&#34;vocab size: {vocab_size}&#34;) # Let there be Autograd, to recursively apply the chain rule through a computation graph class Value: __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage def __init__(self, data, children=(), local_grads=()): self.data = data # scalar value of this node calculated during forward pass self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass self._children = children # children of this node in the computation graph self._local_grads = local_grads # local derivative of this node w.r.t. its children def __add__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data + other.data, (self, other), (1, 1)) def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data * other.data, (self, other), (other.data, self.data)) def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),)) def log(self): return Value(math.log(self.data), (self,), (1/self.data,)) def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),)) def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),)) def __neg__(self): return self * -1 def __radd__(self, other): return self + other def __sub__(self, other): return self + (-other) def __rsub__(self, other): return other + (-self) def __rmul__(self, other): return self * other def __truediv__(self, other): return self * other**-1 def __rtruediv__(self, other): return other * self**-1 def backward(self): topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._children: build_topo(child) topo.append(v) build_topo(self) self.grad = 1 for v in reversed(topo): for child, local_grad in zip(v._children, v._local_grads): child.grad += local_grad * v.grad # Initialize the parameters, to store the knowledge of the model. n_embd = 16 # embedding dimension n_head = 4 # number of attention heads n_layer = 1 # number of layers block_size = 16 # maximum sequence length head_dim = n_embd // n_head # dimension of each head matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)] state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)} for i in range(n_layer): state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd) state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd) params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value] print(f&#34;num params: {len(params)}&#34;) # Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next. # Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU def linear(x, w): return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w] def softmax(logits): max_val = max(val.data for val in logits) exps = [(val - max_val).exp() for val in logits] total = sum(exps) return [e / total for e in exps] def rmsnorm(x): ms = sum(xi * xi for xi in x) / len(x) scale = (ms + 1e-5) ** -0.5 return [xi * scale for xi in x] def gpt(token_id, pos_id, keys, values): tok_emb = state_dict['wte'][token_id] # token embedding pos_emb = state_dict['wpe'][pos_id] # position embedding x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding x = rmsnorm(x) for li in range(n_layer): # 1) Multi-head attention block x_residual = x x = rmsnorm(x) q = linear(x, state_dict[f'layer{li}.attn_wq']) k = linear(x, state_dict[f'layer{li}.attn_wk']) v = linear(x, state_dict[f'layer{li}.attn_wv']) keys[li].append(k) values[li].append(v) x_attn = [] for h in range(n_head): hs = h * head_dim q_h = q[hs:hs+head_dim] k_h = [ki[hs:hs+head_dim] for ki in keys[li]] v_h = [vi[hs:hs+head_dim] for vi in values[li]] attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))] attn_weights = softmax(attn_logits) head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)] x_attn.extend(head_out) x = linear(x_attn, state_dict[f'layer{li}.attn_wo']) x = [a + b for a, b in zip(x, x_residual)] # 2) MLP block x_residual = x x = rmsnorm(x) x = linear(x, state_dict[f'layer{li}.mlp_fc1']) x = [xi.relu() for xi in x] x = linear(x, state_dict[f'layer{li}.mlp_fc2']) x = [a + b for a, b in zip(x, x_residual)] logits = linear(x, state_dict['lm_head']) return logits # Let there be Adam, the blessed optimizer and its buffers learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8 m = [0.0] * len(params) # first moment buffer v = [0.0] * len(params) # second moment buffer # Repeat in sequence num_steps = 1000 # number of training steps for step in range(num_steps): # Take single document, tokenize it, surround it with BOS special token on both sides doc = docs[step % len(docs)] tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS] n = min(block_size, len(tokens) - 1) # Forward the token sequence through the model, building up the computation graph all the way to the loss. keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] losses = [] for pos_id in range(n): token_id, target_id = tokens[pos_id], tokens[pos_id + 1] logits = gpt(token_id, pos_id, keys, values) probs = softmax(logits) loss_t = -probs[target_id].log() losses.append(loss_t) loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low. # Backward the loss, calculating the gradients with respect to all model parameters. loss.backward() # Adam optimizer update: update the model parameters based on the corresponding gradients. lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay for i, p in enumerate(params): m[i] = beta1 * m[i] + (1 - beta1) * p.grad v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2 m_hat = m[i] / (1 - beta1 ** (step + 1)) v_hat = v[i] / (1 - beta2 ** (step + 1)) p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam) p.grad = 0 print(f&#34;step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}&#34;) # Inference: may the model babble back to us temperature = 0.5 # in (0, 1], control the &#34;creativity&#34; of generated text, low to high print(&#34;\n--- inference (new, hallucinated names) ---&#34;) for sample_idx in range(20): keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] token_id = BOS sample = [] for pos_id in range(block_size): logits = gpt(token_id, pos_id, keys, values) probs = softmax([l / temperature for l in logits]) token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0] if token_id == BOS: break sample.append(uchars[token_id]) print(f&#34;sample {sample_idx+1:2d}: {''.join(sample)}&#34;) Line-by-line plain English Line 1: &#34;&#34;&#34;"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-15T21:47:02+00:00"><meta property="article:modified_time" content="2026-02-15T21:47:02+00:00"><meta property="article:tag" content="GPT"><meta property="article:tag" content="Karpathy"><meta property="article:tag" content="Python"><meta property="article:tag" content="Explained"><meta name=twitter:card content="summary"><meta name=twitter:title content="microgpt.py explained in plain English (line by line)"><meta name=twitter:description content="microgpt.py explained in plain English (line by line)
This post rewrites the code line by line into plain English. The original code is by Andrej Karpathy (gist link below).

Original gist: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95


The code
&#34;&#34;&#34;
The most atomic way to train and inference a GPT in pure, dependency-free Python.
This file is the complete algorithm.
Everything else is just efficiency.

@karpathy
&#34;&#34;&#34;

import os       # os.path.exists
import math     # math.log, math.exp
import random   # random.seed, random.choices, random.gauss, random.shuffle
random.seed(42) # Let there be order among chaos

# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)
if not os.path.exists('input.txt'):
    import urllib.request
    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'
    urllib.request.urlretrieve(names_url, 'input.txt')
docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # list[str] of documents
random.shuffle(docs)
print(f&#34;num docs: {len(docs)}&#34;)

# Let there be a Tokenizer to translate strings to discrete symbols and back
uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1
BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token
vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS
print(f&#34;vocab size: {vocab_size}&#34;)

# Let there be Autograd, to recursively apply the chain rule through a computation graph
class Value:
    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage

    def __init__(self, data, children=(), local_grads=()):
        self.data = data                # scalar value of this node calculated during forward pass
        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass
        self._children = children       # children of this node in the computation graph
        self._local_grads = local_grads # local derivative of this node w.r.t. its children

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))
    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))
    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))
    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))
    def __neg__(self): return self * -1
    def __radd__(self, other): return self + other
    def __sub__(self, other): return self + (-other)
    def __rsub__(self, other): return other + (-self)
    def __rmul__(self, other): return self * other
    def __truediv__(self, other): return self * other**-1
    def __rtruediv__(self, other): return other * self**-1

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad

# Initialize the parameters, to store the knowledge of the model.
n_embd = 16     # embedding dimension
n_head = 4      # number of attention heads
n_layer = 1     # number of layers
block_size = 16 # maximum sequence length
head_dim = n_embd // n_head # dimension of each head
matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]
state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}
for i in range(n_layer):
    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)
    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)
params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]
print(f&#34;num params: {len(params)}&#34;)

# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.
# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU
def linear(x, w):
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]

def softmax(logits):
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]

def rmsnorm(x):
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]

def gpt(token_id, pos_id, keys, values):
    tok_emb = state_dict['wte'][token_id] # token embedding
    pos_emb = state_dict['wpe'][pos_id] # position embedding
    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding
    x = rmsnorm(x)

    for li in range(n_layer):
        # 1) Multi-head attention block
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f'layer{li}.attn_wq'])
        k = linear(x, state_dict[f'layer{li}.attn_wk'])
        v = linear(x, state_dict[f'layer{li}.attn_wv'])
        keys[li].append(k)
        values[li].append(v)
        x_attn = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs:hs+head_dim]
            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]
            v_h = [vi[hs:hs+head_dim] for vi in values[li]]
            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]
            attn_weights = softmax(attn_logits)
            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]
            x_attn.extend(head_out)
        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])
        x = [a + b for a, b in zip(x, x_residual)]
        # 2) MLP block
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])
        x = [xi.relu() for xi in x]
        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])
        x = [a + b for a, b in zip(x, x_residual)]

    logits = linear(x, state_dict['lm_head'])
    return logits

# Let there be Adam, the blessed optimizer and its buffers
learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8
m = [0.0] * len(params) # first moment buffer
v = [0.0] * len(params) # second moment buffer

# Repeat in sequence
num_steps = 1000 # number of training steps
for step in range(num_steps):

    # Take single document, tokenize it, surround it with BOS special token on both sides
    doc = docs[step % len(docs)]
    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]
    n = min(block_size, len(tokens) - 1)

    # Forward the token sequence through the model, building up the computation graph all the way to the loss.
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    losses = []
    for pos_id in range(n):
        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax(logits)
        loss_t = -probs[target_id].log()
        losses.append(loss_t)
    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.

    # Backward the loss, calculating the gradients with respect to all model parameters.
    loss.backward()

    # Adam optimizer update: update the model parameters based on the corresponding gradients.
    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay
    for i, p in enumerate(params):
        m[i] = beta1 * m[i] + (1 - beta1) * p.grad
        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2
        m_hat = m[i] / (1 - beta1 ** (step + 1))
        v_hat = v[i] / (1 - beta2 ** (step + 1))
        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)
        p.grad = 0

    print(f&#34;step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}&#34;)

# Inference: may the model babble back to us
temperature = 0.5 # in (0, 1], control the &#34;creativity&#34; of generated text, low to high
print(&#34;\n--- inference (new, hallucinated names) ---&#34;)
for sample_idx in range(20):
    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]
    token_id = BOS
    sample = []
    for pos_id in range(block_size):
        logits = gpt(token_id, pos_id, keys, values)
        probs = softmax([l / temperature for l in logits])
        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]
        if token_id == BOS:
            break
        sample.append(uchars[token_id])
    print(f&#34;sample {sample_idx+1:2d}: {''.join(sample)}&#34;)

Line-by-line plain English
Line 1: &#34;&#34;&#34;"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://emino.app/posts/"},{"@type":"ListItem","position":2,"name":"microgpt.py explained in plain English (line by line)","item":"https://emino.app/posts/microgpt-line-by-line-plain-english/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"microgpt.py explained in plain English (line by line)","name":"microgpt.py explained in plain English (line by line)","description":"microgpt.py explained in plain English (line by line) This post rewrites the code line by line into plain English. The original code is by Andrej Karpathy (gist link below).\nOriginal gist: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95 The code \u0026#34;\u0026#34;\u0026#34; The most atomic way to train and inference a GPT in pure, dependency-free Python. This file is the complete algorithm. Everything else is just efficiency. @karpathy \u0026#34;\u0026#34;\u0026#34; import os # os.path.exists import math # math.log, math.exp import random # random.seed, random.choices, random.gauss, random.shuffle random.seed(42) # Let there be order among chaos # Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names) if not os.path.exists(\u0026#39;input.txt\u0026#39;): import urllib.request names_url = \u0026#39;https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\u0026#39; urllib.request.urlretrieve(names_url, \u0026#39;input.txt\u0026#39;) docs = [l.strip() for l in open(\u0026#39;input.txt\u0026#39;).read().strip().split(\u0026#39;\\n\u0026#39;) if l.strip()] # list[str] of documents random.shuffle(docs) print(f\u0026#34;num docs: {len(docs)}\u0026#34;) # Let there be a Tokenizer to translate strings to discrete symbols and back uchars = sorted(set(\u0026#39;\u0026#39;.join(docs))) # unique characters in the dataset become token ids 0..n-1 BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS print(f\u0026#34;vocab size: {vocab_size}\u0026#34;) # Let there be Autograd, to recursively apply the chain rule through a computation graph class Value: __slots__ = (\u0026#39;data\u0026#39;, \u0026#39;grad\u0026#39;, \u0026#39;_children\u0026#39;, \u0026#39;_local_grads\u0026#39;) # Python optimization for memory usage def __init__(self, data, children=(), local_grads=()): self.data = data # scalar value of this node calculated during forward pass self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass self._children = children # children of this node in the computation graph self._local_grads = local_grads # local derivative of this node w.r.t. its children def __add__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data + other.data, (self, other), (1, 1)) def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data * other.data, (self, other), (other.data, self.data)) def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),)) def log(self): return Value(math.log(self.data), (self,), (1/self.data,)) def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),)) def relu(self): return Value(max(0, self.data), (self,), (float(self.data \u0026gt; 0),)) def __neg__(self): return self * -1 def __radd__(self, other): return self + other def __sub__(self, other): return self + (-other) def __rsub__(self, other): return other + (-self) def __rmul__(self, other): return self * other def __truediv__(self, other): return self * other**-1 def __rtruediv__(self, other): return other * self**-1 def backward(self): topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._children: build_topo(child) topo.append(v) build_topo(self) self.grad = 1 for v in reversed(topo): for child, local_grad in zip(v._children, v._local_grads): child.grad += local_grad * v.grad # Initialize the parameters, to store the knowledge of the model. n_embd = 16 # embedding dimension n_head = 4 # number of attention heads n_layer = 1 # number of layers block_size = 16 # maximum sequence length head_dim = n_embd // n_head # dimension of each head matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)] state_dict = {\u0026#39;wte\u0026#39;: matrix(vocab_size, n_embd), \u0026#39;wpe\u0026#39;: matrix(block_size, n_embd), \u0026#39;lm_head\u0026#39;: matrix(vocab_size, n_embd)} for i in range(n_layer): state_dict[f\u0026#39;layer{i}.attn_wq\u0026#39;] = matrix(n_embd, n_embd) state_dict[f\u0026#39;layer{i}.attn_wk\u0026#39;] = matrix(n_embd, n_embd) state_dict[f\u0026#39;layer{i}.attn_wv\u0026#39;] = matrix(n_embd, n_embd) state_dict[f\u0026#39;layer{i}.attn_wo\u0026#39;] = matrix(n_embd, n_embd) state_dict[f\u0026#39;layer{i}.mlp_fc1\u0026#39;] = matrix(4 * n_embd, n_embd) state_dict[f\u0026#39;layer{i}.mlp_fc2\u0026#39;] = matrix(n_embd, 4 * n_embd) params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value] print(f\u0026#34;num params: {len(params)}\u0026#34;) # Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next. # Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -\u0026gt; rmsnorm, no biases, GeLU -\u0026gt; ReLU def linear(x, w): return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w] def softmax(logits): max_val = max(val.data for val in logits) exps = [(val - max_val).exp() for val in logits] total = sum(exps) return [e / total for e in exps] def rmsnorm(x): ms = sum(xi * xi for xi in x) / len(x) scale = (ms + 1e-5) ** -0.5 return [xi * scale for xi in x] def gpt(token_id, pos_id, keys, values): tok_emb = state_dict[\u0026#39;wte\u0026#39;][token_id] # token embedding pos_emb = state_dict[\u0026#39;wpe\u0026#39;][pos_id] # position embedding x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding x = rmsnorm(x) for li in range(n_layer): # 1) Multi-head attention block x_residual = x x = rmsnorm(x) q = linear(x, state_dict[f\u0026#39;layer{li}.attn_wq\u0026#39;]) k = linear(x, state_dict[f\u0026#39;layer{li}.attn_wk\u0026#39;]) v = linear(x, state_dict[f\u0026#39;layer{li}.attn_wv\u0026#39;]) keys[li].append(k) values[li].append(v) x_attn = [] for h in range(n_head): hs = h * head_dim q_h = q[hs:hs+head_dim] k_h = [ki[hs:hs+head_dim] for ki in keys[li]] v_h = [vi[hs:hs+head_dim] for vi in values[li]] attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))] attn_weights = softmax(attn_logits) head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)] x_attn.extend(head_out) x = linear(x_attn, state_dict[f\u0026#39;layer{li}.attn_wo\u0026#39;]) x = [a + b for a, b in zip(x, x_residual)] # 2) MLP block x_residual = x x = rmsnorm(x) x = linear(x, state_dict[f\u0026#39;layer{li}.mlp_fc1\u0026#39;]) x = [xi.relu() for xi in x] x = linear(x, state_dict[f\u0026#39;layer{li}.mlp_fc2\u0026#39;]) x = [a + b for a, b in zip(x, x_residual)] logits = linear(x, state_dict[\u0026#39;lm_head\u0026#39;]) return logits # Let there be Adam, the blessed optimizer and its buffers learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8 m = [0.0] * len(params) # first moment buffer v = [0.0] * len(params) # second moment buffer # Repeat in sequence num_steps = 1000 # number of training steps for step in range(num_steps): # Take single document, tokenize it, surround it with BOS special token on both sides doc = docs[step % len(docs)] tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS] n = min(block_size, len(tokens) - 1) # Forward the token sequence through the model, building up the computation graph all the way to the loss. keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] losses = [] for pos_id in range(n): token_id, target_id = tokens[pos_id], tokens[pos_id + 1] logits = gpt(token_id, pos_id, keys, values) probs = softmax(logits) loss_t = -probs[target_id].log() losses.append(loss_t) loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low. # Backward the loss, calculating the gradients with respect to all model parameters. loss.backward() # Adam optimizer update: update the model parameters based on the corresponding gradients. lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay for i, p in enumerate(params): m[i] = beta1 * m[i] + (1 - beta1) * p.grad v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2 m_hat = m[i] / (1 - beta1 ** (step + 1)) v_hat = v[i] / (1 - beta2 ** (step + 1)) p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam) p.grad = 0 print(f\u0026#34;step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\u0026#34;) # Inference: may the model babble back to us temperature = 0.5 # in (0, 1], control the \u0026#34;creativity\u0026#34; of generated text, low to high print(\u0026#34;\\n--- inference (new, hallucinated names) ---\u0026#34;) for sample_idx in range(20): keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] token_id = BOS sample = [] for pos_id in range(block_size): logits = gpt(token_id, pos_id, keys, values) probs = softmax([l / temperature for l in logits]) token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0] if token_id == BOS: break sample.append(uchars[token_id]) print(f\u0026#34;sample {sample_idx+1:2d}: {\u0026#39;\u0026#39;.join(sample)}\u0026#34;) Line-by-line plain English Line 1: \u0026quot;\u0026quot;\u0026quot;\n","keywords":["GPT","Karpathy","Python","Explained"],"articleBody":"microgpt.py explained in plain English (line by line) This post rewrites the code line by line into plain English. The original code is by Andrej Karpathy (gist link below).\nOriginal gist: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95 The code \"\"\" The most atomic way to train and inference a GPT in pure, dependency-free Python. This file is the complete algorithm. Everything else is just efficiency. @karpathy \"\"\" import os # os.path.exists import math # math.log, math.exp import random # random.seed, random.choices, random.gauss, random.shuffle random.seed(42) # Let there be order among chaos # Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names) if not os.path.exists('input.txt'): import urllib.request names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt' urllib.request.urlretrieve(names_url, 'input.txt') docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents random.shuffle(docs) print(f\"num docs: {len(docs)}\") # Let there be a Tokenizer to translate strings to discrete symbols and back uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1 BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS print(f\"vocab size: {vocab_size}\") # Let there be Autograd, to recursively apply the chain rule through a computation graph class Value: __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage def __init__(self, data, children=(), local_grads=()): self.data = data # scalar value of this node calculated during forward pass self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass self._children = children # children of this node in the computation graph self._local_grads = local_grads # local derivative of this node w.r.t. its children def __add__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data + other.data, (self, other), (1, 1)) def __mul__(self, other): other = other if isinstance(other, Value) else Value(other) return Value(self.data * other.data, (self, other), (other.data, self.data)) def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),)) def log(self): return Value(math.log(self.data), (self,), (1/self.data,)) def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),)) def relu(self): return Value(max(0, self.data), (self,), (float(self.data \u003e 0),)) def __neg__(self): return self * -1 def __radd__(self, other): return self + other def __sub__(self, other): return self + (-other) def __rsub__(self, other): return other + (-self) def __rmul__(self, other): return self * other def __truediv__(self, other): return self * other**-1 def __rtruediv__(self, other): return other * self**-1 def backward(self): topo = [] visited = set() def build_topo(v): if v not in visited: visited.add(v) for child in v._children: build_topo(child) topo.append(v) build_topo(self) self.grad = 1 for v in reversed(topo): for child, local_grad in zip(v._children, v._local_grads): child.grad += local_grad * v.grad # Initialize the parameters, to store the knowledge of the model. n_embd = 16 # embedding dimension n_head = 4 # number of attention heads n_layer = 1 # number of layers block_size = 16 # maximum sequence length head_dim = n_embd // n_head # dimension of each head matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)] state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)} for i in range(n_layer): state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd) state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd) state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd) params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value] print(f\"num params: {len(params)}\") # Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next. # Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -\u003e rmsnorm, no biases, GeLU -\u003e ReLU def linear(x, w): return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w] def softmax(logits): max_val = max(val.data for val in logits) exps = [(val - max_val).exp() for val in logits] total = sum(exps) return [e / total for e in exps] def rmsnorm(x): ms = sum(xi * xi for xi in x) / len(x) scale = (ms + 1e-5) ** -0.5 return [xi * scale for xi in x] def gpt(token_id, pos_id, keys, values): tok_emb = state_dict['wte'][token_id] # token embedding pos_emb = state_dict['wpe'][pos_id] # position embedding x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding x = rmsnorm(x) for li in range(n_layer): # 1) Multi-head attention block x_residual = x x = rmsnorm(x) q = linear(x, state_dict[f'layer{li}.attn_wq']) k = linear(x, state_dict[f'layer{li}.attn_wk']) v = linear(x, state_dict[f'layer{li}.attn_wv']) keys[li].append(k) values[li].append(v) x_attn = [] for h in range(n_head): hs = h * head_dim q_h = q[hs:hs+head_dim] k_h = [ki[hs:hs+head_dim] for ki in keys[li]] v_h = [vi[hs:hs+head_dim] for vi in values[li]] attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))] attn_weights = softmax(attn_logits) head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)] x_attn.extend(head_out) x = linear(x_attn, state_dict[f'layer{li}.attn_wo']) x = [a + b for a, b in zip(x, x_residual)] # 2) MLP block x_residual = x x = rmsnorm(x) x = linear(x, state_dict[f'layer{li}.mlp_fc1']) x = [xi.relu() for xi in x] x = linear(x, state_dict[f'layer{li}.mlp_fc2']) x = [a + b for a, b in zip(x, x_residual)] logits = linear(x, state_dict['lm_head']) return logits # Let there be Adam, the blessed optimizer and its buffers learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8 m = [0.0] * len(params) # first moment buffer v = [0.0] * len(params) # second moment buffer # Repeat in sequence num_steps = 1000 # number of training steps for step in range(num_steps): # Take single document, tokenize it, surround it with BOS special token on both sides doc = docs[step % len(docs)] tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS] n = min(block_size, len(tokens) - 1) # Forward the token sequence through the model, building up the computation graph all the way to the loss. keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] losses = [] for pos_id in range(n): token_id, target_id = tokens[pos_id], tokens[pos_id + 1] logits = gpt(token_id, pos_id, keys, values) probs = softmax(logits) loss_t = -probs[target_id].log() losses.append(loss_t) loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low. # Backward the loss, calculating the gradients with respect to all model parameters. loss.backward() # Adam optimizer update: update the model parameters based on the corresponding gradients. lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay for i, p in enumerate(params): m[i] = beta1 * m[i] + (1 - beta1) * p.grad v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2 m_hat = m[i] / (1 - beta1 ** (step + 1)) v_hat = v[i] / (1 - beta2 ** (step + 1)) p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam) p.grad = 0 print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\") # Inference: may the model babble back to us temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high print(\"\\n--- inference (new, hallucinated names) ---\") for sample_idx in range(20): keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)] token_id = BOS sample = [] for pos_id in range(block_size): logits = gpt(token_id, pos_id, keys, values) probs = softmax([l / temperature for l in logits]) token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0] if token_id == BOS: break sample.append(uchars[token_id]) print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\") Line-by-line plain English Line 1: \"\"\"\nDo this step as written (part of the algorithm’s mechanics). Line 2: The most atomic way to train and inference a GPT in pure, dependency-free Python.\nDo this step as written (part of the algorithm’s mechanics). Line 3: This file is the complete algorithm.\nDo this step as written (part of the algorithm’s mechanics). Line 4: Everything else is just efficiency.\nDo this step as written (part of the algorithm’s mechanics). Line 5: ``\n(blank line) Line 6: @karpathy\nDo this step as written (part of the algorithm’s mechanics). Line 7: \"\"\"\nDo this step as written (part of the algorithm’s mechanics). Line 8: ``\n(blank line) Line 9: import os # os.path.exists\nImport the built-in Python module os so we can use it later. (Note: os.path.exists) Line 10: import math # math.log, math.exp\nImport the built-in Python module math so we can use it later. (Note: math.log, math.exp) Line 11: import random # random.seed, random.choices, random.gauss, random.shuffle\nImport the built-in Python module random so we can use it later. (Note: random.seed, random.choices, random.gauss, random.shuffle) Line 12: random.seed(42) # Let there be order among chaos\nSet the random seed so results are reproducible (same random choices every run). Line 13: ``\n(blank line) Line 14: # Let there be an input dataset \\docs`: list[str] of documents (e.g. a dataset of names)`\nLet there be an input dataset docs: list[str] of documents (e.g. a dataset of names) Line 15: if not os.path.exists('input.txt'):\nIf the file does not exist yet, download or create it so we have input data. Line 16: import urllib.request\nImport the built-in Python module urllib.request so we can use it later. Line 17: names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\nSet/compute names_url (store a value in a variable). Line 18: urllib.request.urlretrieve(names_url, 'input.txt')\nDownload a file from the internet and save it locally. Line 19: docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents\nRead the input file, split it into lines, clean them up, and store them as a list of documents (strings). Line 20: random.shuffle(docs)\nShuffle the documents so training sees them in random order. Line 21: print(f\"num docs: {len(docs)}\")\nPrint some information to the console so we can see what’s happening. Line 22: ``\n(blank line) Line 23: # Let there be a Tokenizer to translate strings to discrete symbols and back\nLet there be a Tokenizer to translate strings to discrete symbols and back Line 24: uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\nBuild the vocabulary: collect all unique characters used in the dataset and sort them. Line 25: BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token\nPick a special token id that represents “Beginning Of Sequence” (BOS). Line 26: vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\nCompute the total vocabulary size (all characters plus the BOS token). Line 27: print(f\"vocab size: {vocab_size}\")\nPrint some information to the console so we can see what’s happening. Line 28: ``\n(blank line) Line 29: # Let there be Autograd, to recursively apply the chain rule through a computation graph\nLet there be Autograd, to recursively apply the chain rule through a computation graph Line 30: class Value:\nDefine a new class called Value (a custom data type). Line 31: __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\nSet/compute __slots__ (store a value in a variable). Line 32: ``\n(blank line) Line 33: def __init__(self, data, children=(), local_grads=()):\nDefine a function named __init__ (a reusable block of code). Line 34: self.data = data # scalar value of this node calculated during forward pass\nSet/compute self.data (store a value in a variable). Line 35: self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass\nSet/compute self.grad (store a value in a variable). Line 36: self._children = children # children of this node in the computation graph\nSet/compute self._children (store a value in a variable). Line 37: self._local_grads = local_grads # local derivative of this node w.r.t. its children\nSet/compute self._local_grads (store a value in a variable). Line 38: ``\n(blank line) Line 39: def __add__(self, other):\nDefine a function named __add__ (a reusable block of code). Line 40: other = other if isinstance(other, Value) else Value(other)\nSet/compute other (store a value in a variable). Line 41: return Value(self.data + other.data, (self, other), (1, 1))\nReturn a value from this function. Line 42: ``\n(blank line) Line 43: def __mul__(self, other):\nDefine a function named __mul__ (a reusable block of code). Line 44: other = other if isinstance(other, Value) else Value(other)\nSet/compute other (store a value in a variable). Line 45: return Value(self.data * other.data, (self, other), (other.data, self.data))\nReturn a value from this function. Line 46: ``\n(blank line) Line 47: def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\nDefine a function named __pow__ (a reusable block of code). Line 48: def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\nDefine a function named log (a reusable block of code). Line 49: def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\nDefine a function named exp (a reusable block of code). Line 50: def relu(self): return Value(max(0, self.data), (self,), (float(self.data \u003e 0),))\nDefine a function named relu (a reusable block of code). Line 51: def __neg__(self): return self * -1\nDefine a function named __neg__ (a reusable block of code). Line 52: def __radd__(self, other): return self + other\nDefine a function named __radd__ (a reusable block of code). Line 53: def __sub__(self, other): return self + (-other)\nDefine a function named __sub__ (a reusable block of code). Line 54: def __rsub__(self, other): return other + (-self)\nDefine a function named __rsub__ (a reusable block of code). Line 55: def __rmul__(self, other): return self * other\nDefine a function named __rmul__ (a reusable block of code). Line 56: def __truediv__(self, other): return self * other**-1\nDefine a function named __truediv__ (a reusable block of code). Line 57: def __rtruediv__(self, other): return other * self**-1\nDefine a function named __rtruediv__ (a reusable block of code). Line 58: ``\n(blank line) Line 59: def backward(self):\nDefine a function named backward (a reusable block of code). Line 60: topo = []\nSet/compute topo (store a value in a variable). Line 61: visited = set()\nSet/compute visited (store a value in a variable). Line 62: def build_topo(v):\nDefine a function named build_topo (a reusable block of code). Line 63: if v not in visited:\nCheck a condition; if it’s true, run the indented block below. Line 64: visited.add(v)\nDo this step as written (part of the algorithm’s mechanics). Line 65: for child in v._children:\nStart a loop: repeat the next indented block for each item in a collection. Line 66: build_topo(child)\nDo this step as written (part of the algorithm’s mechanics). Line 67: topo.append(v)\nDo this step as written (part of the algorithm’s mechanics). Line 68: build_topo(self)\nDo this step as written (part of the algorithm’s mechanics). Line 69: self.grad = 1\nSet/compute self.grad (store a value in a variable). Line 70: for v in reversed(topo):\nStart a loop: repeat the next indented block for each item in a collection. Line 71: for child, local_grad in zip(v._children, v._local_grads):\nStart a loop: repeat the next indented block for each item in a collection. Line 72: child.grad += local_grad * v.grad\nSet/compute child.grad + (store a value in a variable). Line 73: ``\n(blank line) Line 74: # Initialize the parameters, to store the knowledge of the model.\nInitialize the parameters, to store the knowledge of the model. Line 75: n_embd = 16 # embedding dimension\nSet/compute n_embd (store a value in a variable). Line 76: n_head = 4 # number of attention heads\nSet/compute n_head (store a value in a variable). Line 77: n_layer = 1 # number of layers\nSet/compute n_layer (store a value in a variable). Line 78: block_size = 16 # maximum sequence length\nSet/compute block_size (store a value in a variable). Line 79: head_dim = n_embd // n_head # dimension of each head\nSet/compute head_dim (store a value in a variable). Line 80: matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\nSet/compute matrix (store a value in a variable). Line 81: state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\nSet/compute state_dict (store a value in a variable). Line 82: for i in range(n_layer):\nStart a loop: repeat the next indented block for each item in a collection. Line 83: state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\nSet/compute state_dict[f'layer{i}.attn_wq'] (store a value in a variable). Line 84: state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\nSet/compute state_dict[f'layer{i}.attn_wk'] (store a value in a variable). Line 85: state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\nSet/compute state_dict[f'layer{i}.attn_wv'] (store a value in a variable). Line 86: state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\nSet/compute state_dict[f'layer{i}.attn_wo'] (store a value in a variable). Line 87: state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\nSet/compute state_dict[f'layer{i}.mlp_fc1'] (store a value in a variable). Line 88: state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\nSet/compute state_dict[f'layer{i}.mlp_fc2'] (store a value in a variable). Line 89: params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\nSet/compute params (store a value in a variable). Line 90: print(f\"num params: {len(params)}\")\nPrint some information to the console so we can see what’s happening. Line 91: ``\n(blank line) Line 92: # Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.\nDefine the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next. Line 93: # Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -\u003e rmsnorm, no biases, GeLU -\u003e ReLU\nFollow GPT-2, blessed among the GPTs, with minor differences: layernorm -\u003e rmsnorm, no biases, GeLU -\u003e ReLU Line 94: def linear(x, w):\nDefine a function named linear (a reusable block of code). Line 95: return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\nReturn a value from this function. Line 96: ``\n(blank line) Line 97: def softmax(logits):\nDefine a function named softmax (a reusable block of code). Line 98: max_val = max(val.data for val in logits)\nSet/compute max_val (store a value in a variable). Line 99: exps = [(val - max_val).exp() for val in logits]\nSet/compute exps (store a value in a variable). Line 100: total = sum(exps)\nSet/compute total (store a value in a variable). Line 101: return [e / total for e in exps]\nReturn a value from this function. Line 102: ``\n(blank line) Line 103: def rmsnorm(x):\nDefine a function named rmsnorm (a reusable block of code). Line 104: ms = sum(xi * xi for xi in x) / len(x)\nSet/compute ms (store a value in a variable). Line 105: scale = (ms + 1e-5) ** -0.5\nSet/compute scale (store a value in a variable). Line 106: return [xi * scale for xi in x]\nReturn a value from this function. Line 107: ``\n(blank line) Line 108: def gpt(token_id, pos_id, keys, values):\nDefine a function named gpt (a reusable block of code). Line 109: tok_emb = state_dict['wte'][token_id] # token embedding\nSet/compute tok_emb (store a value in a variable). Line 110: pos_emb = state_dict['wpe'][pos_id] # position embedding\nSet/compute pos_emb (store a value in a variable). Line 111: x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\nSet/compute x (store a value in a variable). Line 112: x = rmsnorm(x)\nSet/compute x (store a value in a variable). Line 113: ``\n(blank line) Line 114: for li in range(n_layer):\nStart a loop: repeat the next indented block for each item in a collection. Line 115: # 1) Multi-head attention block\nMulti-head attention block Line 116: x_residual = x\nSet/compute x_residual (store a value in a variable). Line 117: x = rmsnorm(x)\nSet/compute x (store a value in a variable). Line 118: q = linear(x, state_dict[f'layer{li}.attn_wq'])\nSet/compute q (store a value in a variable). Line 119: k = linear(x, state_dict[f'layer{li}.attn_wk'])\nSet/compute k (store a value in a variable). Line 120: v = linear(x, state_dict[f'layer{li}.attn_wv'])\nSet/compute v (store a value in a variable). Line 121: keys[li].append(k)\nDo this step as written (part of the algorithm’s mechanics). Line 122: values[li].append(v)\nDo this step as written (part of the algorithm’s mechanics). Line 123: x_attn = []\nSet/compute x_attn (store a value in a variable). Line 124: for h in range(n_head):\nStart a loop: repeat the next indented block for each item in a collection. Line 125: hs = h * head_dim\nSet/compute hs (store a value in a variable). Line 126: q_h = q[hs:hs+head_dim]\nSet/compute q_h (store a value in a variable). Line 127: k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\nSet/compute k_h (store a value in a variable). Line 128: v_h = [vi[hs:hs+head_dim] for vi in values[li]]\nSet/compute v_h (store a value in a variable). Line 129: attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\nSet/compute attn_logits (store a value in a variable). Line 130: attn_weights = softmax(attn_logits)\nSet/compute attn_weights (store a value in a variable). Line 131: head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\nSet/compute head_out (store a value in a variable). Line 132: x_attn.extend(head_out)\nDo this step as written (part of the algorithm’s mechanics). Line 133: x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\nSet/compute x (store a value in a variable). Line 134: x = [a + b for a, b in zip(x, x_residual)]\nSet/compute x (store a value in a variable). Line 135: # 2) MLP block\nMLP block Line 136: x_residual = x\nSet/compute x_residual (store a value in a variable). Line 137: x = rmsnorm(x)\nSet/compute x (store a value in a variable). Line 138: x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\nSet/compute x (store a value in a variable). Line 139: x = [xi.relu() for xi in x]\nSet/compute x (store a value in a variable). Line 140: x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\nSet/compute x (store a value in a variable). Line 141: x = [a + b for a, b in zip(x, x_residual)]\nSet/compute x (store a value in a variable). Line 142: ``\n(blank line) Line 143: logits = linear(x, state_dict['lm_head'])\nSet/compute logits (store a value in a variable). Line 144: return logits\nReturn a value from this function. Line 145: ``\n(blank line) Line 146: # Let there be Adam, the blessed optimizer and its buffers\nLet there be Adam, the blessed optimizer and its buffers Line 147: learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\nSet/compute learning_rate, beta1, beta2, eps_adam (store a value in a variable). Line 148: m = [0.0] * len(params) # first moment buffer\nSet/compute m (store a value in a variable). Line 149: v = [0.0] * len(params) # second moment buffer\nSet/compute v (store a value in a variable). Line 150: ``\n(blank line) Line 151: # Repeat in sequence\nRepeat in sequence Line 152: num_steps = 1000 # number of training steps\nSet/compute num_steps (store a value in a variable). Line 153: for step in range(num_steps):\nStart a loop: repeat the next indented block for each item in a collection. Line 154: ``\n(blank line) Line 155: # Take single document, tokenize it, surround it with BOS special token on both sides\nTake single document, tokenize it, surround it with BOS special token on both sides Line 156: doc = docs[step % len(docs)]\nSet/compute doc (store a value in a variable). Line 157: tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\nSet/compute tokens (store a value in a variable). Line 158: n = min(block_size, len(tokens) - 1)\nSet/compute n (store a value in a variable). Line 159: ``\n(blank line) Line 160: # Forward the token sequence through the model, building up the computation graph all the way to the loss.\nForward the token sequence through the model, building up the computation graph all the way to the loss. Line 161: keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nSet/compute keys, values (store a value in a variable). Line 162: losses = []\nSet/compute losses (store a value in a variable). Line 163: for pos_id in range(n):\nStart a loop: repeat the next indented block for each item in a collection. Line 164: token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\nSet/compute token_id, target_id (store a value in a variable). Line 165: logits = gpt(token_id, pos_id, keys, values)\nSet/compute logits (store a value in a variable). Line 166: probs = softmax(logits)\nSet/compute probs (store a value in a variable). Line 167: loss_t = -probs[target_id].log()\nSet/compute loss_t (store a value in a variable). Line 168: losses.append(loss_t)\nDo this step as written (part of the algorithm’s mechanics). Line 169: loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\nSet/compute loss (store a value in a variable). Line 170: ``\n(blank line) Line 171: # Backward the loss, calculating the gradients with respect to all model parameters.\nBackward the loss, calculating the gradients with respect to all model parameters. Line 172: loss.backward()\nDo this step as written (part of the algorithm’s mechanics). Line 173: ``\n(blank line) Line 174: # Adam optimizer update: update the model parameters based on the corresponding gradients.\nAdam optimizer update: update the model parameters based on the corresponding gradients. Line 175: lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\nSet/compute lr_t (store a value in a variable). Line 176: for i, p in enumerate(params):\nStart a loop: repeat the next indented block for each item in a collection. Line 177: m[i] = beta1 * m[i] + (1 - beta1) * p.grad\nSet/compute m[i] (store a value in a variable). Line 178: v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\nSet/compute v[i] (store a value in a variable). Line 179: m_hat = m[i] / (1 - beta1 ** (step + 1))\nSet/compute m_hat (store a value in a variable). Line 180: v_hat = v[i] / (1 - beta2 ** (step + 1))\nSet/compute v_hat (store a value in a variable). Line 181: p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\nSet/compute p.data - (store a value in a variable). Line 182: p.grad = 0\nSet/compute p.grad (store a value in a variable). Line 183: ``\n(blank line) Line 184: print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\nPrint some information to the console so we can see what’s happening. Line 185: ``\n(blank line) Line 186: # Inference: may the model babble back to us\nInference: may the model babble back to us Line 187: temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\nSet/compute temperature (store a value in a variable). Line 188: print(\"\\n--- inference (new, hallucinated names) ---\")\nPrint some information to the console so we can see what’s happening. Line 189: for sample_idx in range(20):\nStart a loop: repeat the next indented block for each item in a collection. Line 190: keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\nSet/compute keys, values (store a value in a variable). Line 191: token_id = BOS\nSet/compute token_id (store a value in a variable). Line 192: sample = []\nSet/compute sample (store a value in a variable). Line 193: for pos_id in range(block_size):\nStart a loop: repeat the next indented block for each item in a collection. Line 194: logits = gpt(token_id, pos_id, keys, values)\nSet/compute logits (store a value in a variable). Line 195: probs = softmax([l / temperature for l in logits])\nSet/compute probs (store a value in a variable). Line 196: token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\nSet/compute token_id (store a value in a variable). Line 197: if token_id == BOS:\nCheck a condition; if it’s true, run the indented block below. Line 198: break\nDo this step as written (part of the algorithm’s mechanics). Line 199: sample.append(uchars[token_id])\nDo this step as written (part of the algorithm’s mechanics). Line 200: print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")\nPrint some information to the console so we can see what’s happening. ","wordCount":"4590","inLanguage":"en","datePublished":"2026-02-15T21:47:02Z","dateModified":"2026-02-15T21:47:02Z","author":{"@type":"Person","name":"Emin Henri Mahrt"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://emino.app/posts/microgpt-line-by-line-plain-english/"},"publisher":{"@type":"Organization","name":"Art \u0026 Articles by Emin Mahrt","logo":{"@type":"ImageObject","url":"https://emino.app/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://emino.app/ accesskey=h title="Art & Articles by Emin Mahrt (Alt + H)">Art & Articles by Emin Mahrt</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://emino.app/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://emino.app/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://emino.app/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://emino.app/>Home</a>&nbsp;»&nbsp;<a href=https://emino.app/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">microgpt.py explained in plain English (line by line)</h1><div class=post-meta><span title='2026-02-15 21:47:02 +0000 UTC'>February 15, 2026</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;Emin Henri Mahrt</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#microgptpy-explained-in-plain-english-line-by-line aria-label="microgpt.py explained in plain English (line by line)">microgpt.py explained in plain English (line by line)</a><ul><li><a href=#the-code aria-label="The code">The code</a></li><li><a href=#line-by-line-plain-english aria-label="Line-by-line plain English">Line-by-line plain English</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=microgptpy-explained-in-plain-english-line-by-line>microgpt.py explained in plain English (line by line)<a hidden class=anchor aria-hidden=true href=#microgptpy-explained-in-plain-english-line-by-line>#</a></h1><p>This post rewrites the code <strong>line by line</strong> into plain English. The original code is by Andrej Karpathy (gist link below).</p><ul><li>Original gist: <a href=https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95>https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95</a></li></ul><hr><h2 id=the-code>The code<a hidden class=anchor aria-hidden=true href=#the-code>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>The most atomic way to train and inference a GPT in pure, dependency-free Python.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>This file is the complete algorithm.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Everything else is just efficiency.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>@karpathy
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os       <span style=color:#75715e># os.path.exists</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> math     <span style=color:#75715e># math.log, math.exp</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random   <span style=color:#75715e># random.seed, random.choices, random.gauss, random.shuffle</span>
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>) <span style=color:#75715e># Let there be order among chaos</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(<span style=color:#e6db74>&#39;input.txt&#39;</span>):
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> urllib.request
</span></span><span style=display:flex><span>    names_url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt&#39;</span>
</span></span><span style=display:flex><span>    urllib<span style=color:#f92672>.</span>request<span style=color:#f92672>.</span>urlretrieve(names_url, <span style=color:#e6db74>&#39;input.txt&#39;</span>)
</span></span><span style=display:flex><span>docs <span style=color:#f92672>=</span> [l<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> open(<span style=color:#e6db74>&#39;input.txt&#39;</span>)<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>) <span style=color:#66d9ef>if</span> l<span style=color:#f92672>.</span>strip()] <span style=color:#75715e># list[str] of documents</span>
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>shuffle(docs)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;num docs: </span><span style=color:#e6db74>{</span>len(docs)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be a Tokenizer to translate strings to discrete symbols and back</span>
</span></span><span style=display:flex><span>uchars <span style=color:#f92672>=</span> sorted(set(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(docs))) <span style=color:#75715e># unique characters in the dataset become token ids 0..n-1</span>
</span></span><span style=display:flex><span>BOS <span style=color:#f92672>=</span> len(uchars) <span style=color:#75715e># token id for the special Beginning of Sequence (BOS) token</span>
</span></span><span style=display:flex><span>vocab_size <span style=color:#f92672>=</span> len(uchars) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span> <span style=color:#75715e># total number of unique tokens, +1 is for BOS</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;vocab size: </span><span style=color:#e6db74>{</span>vocab_size<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be Autograd, to recursively apply the chain rule through a computation graph</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Value</span>:
</span></span><span style=display:flex><span>    __slots__ <span style=color:#f92672>=</span> (<span style=color:#e6db74>&#39;data&#39;</span>, <span style=color:#e6db74>&#39;grad&#39;</span>, <span style=color:#e6db74>&#39;_children&#39;</span>, <span style=color:#e6db74>&#39;_local_grads&#39;</span>) <span style=color:#75715e># Python optimization for memory usage</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, data, children<span style=color:#f92672>=</span>(), local_grads<span style=color:#f92672>=</span>()):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> data                <span style=color:#75715e># scalar value of this node calculated during forward pass</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>                   <span style=color:#75715e># derivative of the loss w.r.t. this node, calculated in backward pass</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_children <span style=color:#f92672>=</span> children       <span style=color:#75715e># children of this node in the computation graph</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_local_grads <span style=color:#f92672>=</span> local_grads <span style=color:#75715e># local derivative of this node w.r.t. its children</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__add__</span>(self, other):
</span></span><span style=display:flex><span>        other <span style=color:#f92672>=</span> other <span style=color:#66d9ef>if</span> isinstance(other, Value) <span style=color:#66d9ef>else</span> Value(other)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Value(self<span style=color:#f92672>.</span>data <span style=color:#f92672>+</span> other<span style=color:#f92672>.</span>data, (self, other), (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__mul__</span>(self, other):
</span></span><span style=display:flex><span>        other <span style=color:#f92672>=</span> other <span style=color:#66d9ef>if</span> isinstance(other, Value) <span style=color:#66d9ef>else</span> Value(other)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Value(self<span style=color:#f92672>.</span>data <span style=color:#f92672>*</span> other<span style=color:#f92672>.</span>data, (self, other), (other<span style=color:#f92672>.</span>data, self<span style=color:#f92672>.</span>data))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__pow__</span>(self, other): <span style=color:#66d9ef>return</span> Value(self<span style=color:#f92672>.</span>data<span style=color:#f92672>**</span>other, (self,), (other <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>data<span style=color:#f92672>**</span>(other<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>),))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>log</span>(self): <span style=color:#66d9ef>return</span> Value(math<span style=color:#f92672>.</span>log(self<span style=color:#f92672>.</span>data), (self,), (<span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>self<span style=color:#f92672>.</span>data,))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>exp</span>(self): <span style=color:#66d9ef>return</span> Value(math<span style=color:#f92672>.</span>exp(self<span style=color:#f92672>.</span>data), (self,), (math<span style=color:#f92672>.</span>exp(self<span style=color:#f92672>.</span>data),))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>relu</span>(self): <span style=color:#66d9ef>return</span> Value(max(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>data), (self,), (float(self<span style=color:#f92672>.</span>data <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>),))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__neg__</span>(self): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__radd__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>+</span> other
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__sub__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>+</span> (<span style=color:#f92672>-</span>other)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__rsub__</span>(self, other): <span style=color:#66d9ef>return</span> other <span style=color:#f92672>+</span> (<span style=color:#f92672>-</span>self)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__rmul__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> other
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__truediv__</span>(self, other): <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> other<span style=color:#f92672>**-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__rtruediv__</span>(self, other): <span style=color:#66d9ef>return</span> other <span style=color:#f92672>*</span> self<span style=color:#f92672>**-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(self):
</span></span><span style=display:flex><span>        topo <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        visited <span style=color:#f92672>=</span> set()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_topo</span>(v):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> v <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>                visited<span style=color:#f92672>.</span>add(v)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> child <span style=color:#f92672>in</span> v<span style=color:#f92672>.</span>_children:
</span></span><span style=display:flex><span>                    build_topo(child)
</span></span><span style=display:flex><span>                topo<span style=color:#f92672>.</span>append(v)
</span></span><span style=display:flex><span>        build_topo(self)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> v <span style=color:#f92672>in</span> reversed(topo):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> child, local_grad <span style=color:#f92672>in</span> zip(v<span style=color:#f92672>.</span>_children, v<span style=color:#f92672>.</span>_local_grads):
</span></span><span style=display:flex><span>                child<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> local_grad <span style=color:#f92672>*</span> v<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize the parameters, to store the knowledge of the model.</span>
</span></span><span style=display:flex><span>n_embd <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span>     <span style=color:#75715e># embedding dimension</span>
</span></span><span style=display:flex><span>n_head <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>      <span style=color:#75715e># number of attention heads</span>
</span></span><span style=display:flex><span>n_layer <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>     <span style=color:#75715e># number of layers</span>
</span></span><span style=display:flex><span>block_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>16</span> <span style=color:#75715e># maximum sequence length</span>
</span></span><span style=display:flex><span>head_dim <span style=color:#f92672>=</span> n_embd <span style=color:#f92672>//</span> n_head <span style=color:#75715e># dimension of each head</span>
</span></span><span style=display:flex><span>matrix <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> nout, nin, std<span style=color:#f92672>=</span><span style=color:#ae81ff>0.08</span>: [[Value(random<span style=color:#f92672>.</span>gauss(<span style=color:#ae81ff>0</span>, std)) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nin)] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nout)]
</span></span><span style=display:flex><span>state_dict <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;wte&#39;</span>: matrix(vocab_size, n_embd), <span style=color:#e6db74>&#39;wpe&#39;</span>: matrix(block_size, n_embd), <span style=color:#e6db74>&#39;lm_head&#39;</span>: matrix(vocab_size, n_embd)}
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_layer):
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wq&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wk&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wv&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wo&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc1&#39;</span>] <span style=color:#f92672>=</span> matrix(<span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> n_embd, n_embd)
</span></span><span style=display:flex><span>    state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc2&#39;</span>] <span style=color:#f92672>=</span> matrix(n_embd, <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> n_embd)
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> [p <span style=color:#66d9ef>for</span> mat <span style=color:#f92672>in</span> state_dict<span style=color:#f92672>.</span>values() <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> mat <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> row] <span style=color:#75715e># flatten params into a single list[Value]</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;num params: </span><span style=color:#e6db74>{</span>len(params)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -&gt; rmsnorm, no biases, GeLU -&gt; ReLU</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linear</span>(x, w):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [sum(wi <span style=color:#f92672>*</span> xi <span style=color:#66d9ef>for</span> wi, xi <span style=color:#f92672>in</span> zip(wo, x)) <span style=color:#66d9ef>for</span> wo <span style=color:#f92672>in</span> w]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(logits):
</span></span><span style=display:flex><span>    max_val <span style=color:#f92672>=</span> max(val<span style=color:#f92672>.</span>data <span style=color:#66d9ef>for</span> val <span style=color:#f92672>in</span> logits)
</span></span><span style=display:flex><span>    exps <span style=color:#f92672>=</span> [(val <span style=color:#f92672>-</span> max_val)<span style=color:#f92672>.</span>exp() <span style=color:#66d9ef>for</span> val <span style=color:#f92672>in</span> logits]
</span></span><span style=display:flex><span>    total <span style=color:#f92672>=</span> sum(exps)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [e <span style=color:#f92672>/</span> total <span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> exps]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>rmsnorm</span>(x):
</span></span><span style=display:flex><span>    ms <span style=color:#f92672>=</span> sum(xi <span style=color:#f92672>*</span> xi <span style=color:#66d9ef>for</span> xi <span style=color:#f92672>in</span> x) <span style=color:#f92672>/</span> len(x)
</span></span><span style=display:flex><span>    scale <span style=color:#f92672>=</span> (ms <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-5</span>) <span style=color:#f92672>**</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [xi <span style=color:#f92672>*</span> scale <span style=color:#66d9ef>for</span> xi <span style=color:#f92672>in</span> x]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gpt</span>(token_id, pos_id, keys, values):
</span></span><span style=display:flex><span>    tok_emb <span style=color:#f92672>=</span> state_dict[<span style=color:#e6db74>&#39;wte&#39;</span>][token_id] <span style=color:#75715e># token embedding</span>
</span></span><span style=display:flex><span>    pos_emb <span style=color:#f92672>=</span> state_dict[<span style=color:#e6db74>&#39;wpe&#39;</span>][pos_id] <span style=color:#75715e># position embedding</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> [t <span style=color:#f92672>+</span> p <span style=color:#66d9ef>for</span> t, p <span style=color:#f92672>in</span> zip(tok_emb, pos_emb)] <span style=color:#75715e># joint token and position embedding</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> rmsnorm(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> li <span style=color:#f92672>in</span> range(n_layer):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1) Multi-head attention block</span>
</span></span><span style=display:flex><span>        x_residual <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> rmsnorm(x)
</span></span><span style=display:flex><span>        q <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wq&#39;</span>])
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wk&#39;</span>])
</span></span><span style=display:flex><span>        v <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wv&#39;</span>])
</span></span><span style=display:flex><span>        keys[li]<span style=color:#f92672>.</span>append(k)
</span></span><span style=display:flex><span>        values[li]<span style=color:#f92672>.</span>append(v)
</span></span><span style=display:flex><span>        x_attn <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> h <span style=color:#f92672>in</span> range(n_head):
</span></span><span style=display:flex><span>            hs <span style=color:#f92672>=</span> h <span style=color:#f92672>*</span> head_dim
</span></span><span style=display:flex><span>            q_h <span style=color:#f92672>=</span> q[hs:hs<span style=color:#f92672>+</span>head_dim]
</span></span><span style=display:flex><span>            k_h <span style=color:#f92672>=</span> [ki[hs:hs<span style=color:#f92672>+</span>head_dim] <span style=color:#66d9ef>for</span> ki <span style=color:#f92672>in</span> keys[li]]
</span></span><span style=display:flex><span>            v_h <span style=color:#f92672>=</span> [vi[hs:hs<span style=color:#f92672>+</span>head_dim] <span style=color:#66d9ef>for</span> vi <span style=color:#f92672>in</span> values[li]]
</span></span><span style=display:flex><span>            attn_logits <span style=color:#f92672>=</span> [sum(q_h[j] <span style=color:#f92672>*</span> k_h[t][j] <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(head_dim)) <span style=color:#f92672>/</span> head_dim<span style=color:#f92672>**</span><span style=color:#ae81ff>0.5</span> <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(len(k_h))]
</span></span><span style=display:flex><span>            attn_weights <span style=color:#f92672>=</span> softmax(attn_logits)
</span></span><span style=display:flex><span>            head_out <span style=color:#f92672>=</span> [sum(attn_weights[t] <span style=color:#f92672>*</span> v_h[t][j] <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(len(v_h))) <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(head_dim)]
</span></span><span style=display:flex><span>            x_attn<span style=color:#f92672>.</span>extend(head_out)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> linear(x_attn, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.attn_wo&#39;</span>])
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [a <span style=color:#f92672>+</span> b <span style=color:#66d9ef>for</span> a, b <span style=color:#f92672>in</span> zip(x, x_residual)]
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2) MLP block</span>
</span></span><span style=display:flex><span>        x_residual <span style=color:#f92672>=</span> x
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> rmsnorm(x)
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc1&#39;</span>])
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [xi<span style=color:#f92672>.</span>relu() <span style=color:#66d9ef>for</span> xi <span style=color:#f92672>in</span> x]
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;layer</span><span style=color:#e6db74>{</span>li<span style=color:#e6db74>}</span><span style=color:#e6db74>.mlp_fc2&#39;</span>])
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> [a <span style=color:#f92672>+</span> b <span style=color:#66d9ef>for</span> a, b <span style=color:#f92672>in</span> zip(x, x_residual)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> linear(x, state_dict[<span style=color:#e6db74>&#39;lm_head&#39;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> logits
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Let there be Adam, the blessed optimizer and its buffers</span>
</span></span><span style=display:flex><span>learning_rate, beta1, beta2, eps_adam <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>, <span style=color:#ae81ff>0.85</span>, <span style=color:#ae81ff>0.99</span>, <span style=color:#ae81ff>1e-8</span>
</span></span><span style=display:flex><span>m <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0.0</span>] <span style=color:#f92672>*</span> len(params) <span style=color:#75715e># first moment buffer</span>
</span></span><span style=display:flex><span>v <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0.0</span>] <span style=color:#f92672>*</span> len(params) <span style=color:#75715e># second moment buffer</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Repeat in sequence</span>
</span></span><span style=display:flex><span>num_steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span> <span style=color:#75715e># number of training steps</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> step <span style=color:#f92672>in</span> range(num_steps):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Take single document, tokenize it, surround it with BOS special token on both sides</span>
</span></span><span style=display:flex><span>    doc <span style=color:#f92672>=</span> docs[step <span style=color:#f92672>%</span> len(docs)]
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> [BOS] <span style=color:#f92672>+</span> [uchars<span style=color:#f92672>.</span>index(ch) <span style=color:#66d9ef>for</span> ch <span style=color:#f92672>in</span> doc] <span style=color:#f92672>+</span> [BOS]
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> min(block_size, len(tokens) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Forward the token sequence through the model, building up the computation graph all the way to the loss.</span>
</span></span><span style=display:flex><span>    keys, values <span style=color:#f92672>=</span> [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)], [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)]
</span></span><span style=display:flex><span>    losses <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> pos_id <span style=color:#f92672>in</span> range(n):
</span></span><span style=display:flex><span>        token_id, target_id <span style=color:#f92672>=</span> tokens[pos_id], tokens[pos_id <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> gpt(token_id, pos_id, keys, values)
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> softmax(logits)
</span></span><span style=display:flex><span>        loss_t <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[target_id]<span style=color:#f92672>.</span>log()
</span></span><span style=display:flex><span>        losses<span style=color:#f92672>.</span>append(loss_t)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> n) <span style=color:#f92672>*</span> sum(losses) <span style=color:#75715e># final average loss over the document sequence. May yours be low.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Backward the loss, calculating the gradients with respect to all model parameters.</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Adam optimizer update: update the model parameters based on the corresponding gradients.</span>
</span></span><span style=display:flex><span>    lr_t <span style=color:#f92672>=</span> learning_rate <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> step <span style=color:#f92672>/</span> num_steps) <span style=color:#75715e># linear learning rate decay</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, p <span style=color:#f92672>in</span> enumerate(params):
</span></span><span style=display:flex><span>        m[i] <span style=color:#f92672>=</span> beta1 <span style=color:#f92672>*</span> m[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta1) <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>        v[i] <span style=color:#f92672>=</span> beta2 <span style=color:#f92672>*</span> v[i] <span style=color:#f92672>+</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta2) <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        m_hat <span style=color:#f92672>=</span> m[i] <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta1 <span style=color:#f92672>**</span> (step <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        v_hat <span style=color:#f92672>=</span> v[i] <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> beta2 <span style=color:#f92672>**</span> (step <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>-=</span> lr_t <span style=color:#f92672>*</span> m_hat <span style=color:#f92672>/</span> (v_hat <span style=color:#f92672>**</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>+</span> eps_adam)
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;step </span><span style=color:#e6db74>{</span>step<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>:</span><span style=color:#e6db74>4d</span><span style=color:#e6db74>}</span><span style=color:#e6db74> / </span><span style=color:#e6db74>{</span>num_steps<span style=color:#e6db74>:</span><span style=color:#e6db74>4d</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | loss </span><span style=color:#e6db74>{</span>loss<span style=color:#f92672>.</span>data<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Inference: may the model babble back to us</span>
</span></span><span style=display:flex><span>temperature <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span> <span style=color:#75715e># in (0, 1], control the &#34;creativity&#34; of generated text, low to high</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>--- inference (new, hallucinated names) ---&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> sample_idx <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>    keys, values <span style=color:#f92672>=</span> [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)], [[] <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_layer)]
</span></span><span style=display:flex><span>    token_id <span style=color:#f92672>=</span> BOS
</span></span><span style=display:flex><span>    sample <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> pos_id <span style=color:#f92672>in</span> range(block_size):
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> gpt(token_id, pos_id, keys, values)
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> softmax([l <span style=color:#f92672>/</span> temperature <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> logits])
</span></span><span style=display:flex><span>        token_id <span style=color:#f92672>=</span> random<span style=color:#f92672>.</span>choices(range(vocab_size), weights<span style=color:#f92672>=</span>[p<span style=color:#f92672>.</span>data <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> probs])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> token_id <span style=color:#f92672>==</span> BOS:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        sample<span style=color:#f92672>.</span>append(uchars[token_id])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;sample </span><span style=color:#e6db74>{</span>sample_idx<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>:</span><span style=color:#e6db74>2d</span><span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span><span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(sample)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><hr><h2 id=line-by-line-plain-english>Line-by-line plain English<a hidden class=anchor aria-hidden=true href=#line-by-line-plain-english>#</a></h2><p><strong>Line 1:</strong> <code>"""</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 2:</strong> <code>The most atomic way to train and inference a GPT in pure, dependency-free Python.</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 3:</strong> <code>This file is the complete algorithm.</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 4:</strong> <code>Everything else is just efficiency.</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 5:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 6:</strong> <code>@karpathy</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 7:</strong> <code>"""</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 8:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 9:</strong> <code>import os # os.path.exists</code></p><ul><li>Import the built-in Python module <code>os</code> so we can use it later. (Note: os.path.exists)</li></ul><p><strong>Line 10:</strong> <code>import math # math.log, math.exp</code></p><ul><li>Import the built-in Python module <code>math</code> so we can use it later. (Note: math.log, math.exp)</li></ul><p><strong>Line 11:</strong> <code>import random # random.seed, random.choices, random.gauss, random.shuffle</code></p><ul><li>Import the built-in Python module <code>random</code> so we can use it later. (Note: random.seed, random.choices, random.gauss, random.shuffle)</li></ul><p><strong>Line 12:</strong> <code>random.seed(42) # Let there be order among chaos</code></p><ul><li>Set the random seed so results are reproducible (same random choices every run).</li></ul><p><strong>Line 13:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 14:</strong> <code># Let there be an input dataset \</code>docs`: list[str] of documents (e.g. a dataset of names)`</p><ul><li>Let there be an input dataset <code>docs</code>: list[str] of documents (e.g. a dataset of names)</li></ul><p><strong>Line 15:</strong> <code>if not os.path.exists('input.txt'):</code></p><ul><li>If the file does not exist yet, download or create it so we have input data.</li></ul><p><strong>Line 16:</strong> <code>import urllib.request</code></p><ul><li>Import the built-in Python module <code>urllib.request</code> so we can use it later.</li></ul><p><strong>Line 17:</strong> <code>names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'</code></p><ul><li>Set/compute <code>names_url</code> (store a value in a variable).</li></ul><p><strong>Line 18:</strong> <code>urllib.request.urlretrieve(names_url, 'input.txt')</code></p><ul><li>Download a file from the internet and save it locally.</li></ul><p><strong>Line 19:</strong> <code>docs = [l.strip() for l in open('input.txt').read().strip().split('\n') if l.strip()] # list[str] of documents</code></p><ul><li>Read the input file, split it into lines, clean them up, and store them as a list of documents (strings).</li></ul><p><strong>Line 20:</strong> <code>random.shuffle(docs)</code></p><ul><li>Shuffle the documents so training sees them in random order.</li></ul><p><strong>Line 21:</strong> <code>print(f"num docs: {len(docs)}")</code></p><ul><li>Print some information to the console so we can see what’s happening.</li></ul><p><strong>Line 22:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 23:</strong> <code># Let there be a Tokenizer to translate strings to discrete symbols and back</code></p><ul><li>Let there be a Tokenizer to translate strings to discrete symbols and back</li></ul><p><strong>Line 24:</strong> <code>uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1</code></p><ul><li>Build the vocabulary: collect all unique characters used in the dataset and sort them.</li></ul><p><strong>Line 25:</strong> <code>BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token</code></p><ul><li>Pick a special token id that represents “Beginning Of Sequence” (BOS).</li></ul><p><strong>Line 26:</strong> <code>vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS</code></p><ul><li>Compute the total vocabulary size (all characters plus the BOS token).</li></ul><p><strong>Line 27:</strong> <code>print(f"vocab size: {vocab_size}")</code></p><ul><li>Print some information to the console so we can see what’s happening.</li></ul><p><strong>Line 28:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 29:</strong> <code># Let there be Autograd, to recursively apply the chain rule through a computation graph</code></p><ul><li>Let there be Autograd, to recursively apply the chain rule through a computation graph</li></ul><p><strong>Line 30:</strong> <code>class Value:</code></p><ul><li>Define a new class called <code>Value</code> (a custom data type).</li></ul><p><strong>Line 31:</strong> <code>__slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage</code></p><ul><li>Set/compute <code>__slots__</code> (store a value in a variable).</li></ul><p><strong>Line 32:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 33:</strong> <code>def __init__(self, data, children=(), local_grads=()):</code></p><ul><li>Define a function named <code>__init__</code> (a reusable block of code).</li></ul><p><strong>Line 34:</strong> <code>self.data = data # scalar value of this node calculated during forward pass</code></p><ul><li>Set/compute <code>self.data</code> (store a value in a variable).</li></ul><p><strong>Line 35:</strong> <code>self.grad = 0 # derivative of the loss w.r.t. this node, calculated in backward pass</code></p><ul><li>Set/compute <code>self.grad</code> (store a value in a variable).</li></ul><p><strong>Line 36:</strong> <code>self._children = children # children of this node in the computation graph</code></p><ul><li>Set/compute <code>self._children</code> (store a value in a variable).</li></ul><p><strong>Line 37:</strong> <code>self._local_grads = local_grads # local derivative of this node w.r.t. its children</code></p><ul><li>Set/compute <code>self._local_grads</code> (store a value in a variable).</li></ul><p><strong>Line 38:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 39:</strong> <code>def __add__(self, other):</code></p><ul><li>Define a function named <code>__add__</code> (a reusable block of code).</li></ul><p><strong>Line 40:</strong> <code>other = other if isinstance(other, Value) else Value(other)</code></p><ul><li>Set/compute <code>other</code> (store a value in a variable).</li></ul><p><strong>Line 41:</strong> <code>return Value(self.data + other.data, (self, other), (1, 1))</code></p><ul><li>Return a value from this function.</li></ul><p><strong>Line 42:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 43:</strong> <code>def __mul__(self, other):</code></p><ul><li>Define a function named <code>__mul__</code> (a reusable block of code).</li></ul><p><strong>Line 44:</strong> <code>other = other if isinstance(other, Value) else Value(other)</code></p><ul><li>Set/compute <code>other</code> (store a value in a variable).</li></ul><p><strong>Line 45:</strong> <code>return Value(self.data * other.data, (self, other), (other.data, self.data))</code></p><ul><li>Return a value from this function.</li></ul><p><strong>Line 46:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 47:</strong> <code>def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))</code></p><ul><li>Define a function named <code>__pow__</code> (a reusable block of code).</li></ul><p><strong>Line 48:</strong> <code>def log(self): return Value(math.log(self.data), (self,), (1/self.data,))</code></p><ul><li>Define a function named <code>log</code> (a reusable block of code).</li></ul><p><strong>Line 49:</strong> <code>def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))</code></p><ul><li>Define a function named <code>exp</code> (a reusable block of code).</li></ul><p><strong>Line 50:</strong> <code>def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))</code></p><ul><li>Define a function named <code>relu</code> (a reusable block of code).</li></ul><p><strong>Line 51:</strong> <code>def __neg__(self): return self * -1</code></p><ul><li>Define a function named <code>__neg__</code> (a reusable block of code).</li></ul><p><strong>Line 52:</strong> <code>def __radd__(self, other): return self + other</code></p><ul><li>Define a function named <code>__radd__</code> (a reusable block of code).</li></ul><p><strong>Line 53:</strong> <code>def __sub__(self, other): return self + (-other)</code></p><ul><li>Define a function named <code>__sub__</code> (a reusable block of code).</li></ul><p><strong>Line 54:</strong> <code>def __rsub__(self, other): return other + (-self)</code></p><ul><li>Define a function named <code>__rsub__</code> (a reusable block of code).</li></ul><p><strong>Line 55:</strong> <code>def __rmul__(self, other): return self * other</code></p><ul><li>Define a function named <code>__rmul__</code> (a reusable block of code).</li></ul><p><strong>Line 56:</strong> <code>def __truediv__(self, other): return self * other**-1</code></p><ul><li>Define a function named <code>__truediv__</code> (a reusable block of code).</li></ul><p><strong>Line 57:</strong> <code>def __rtruediv__(self, other): return other * self**-1</code></p><ul><li>Define a function named <code>__rtruediv__</code> (a reusable block of code).</li></ul><p><strong>Line 58:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 59:</strong> <code>def backward(self):</code></p><ul><li>Define a function named <code>backward</code> (a reusable block of code).</li></ul><p><strong>Line 60:</strong> <code>topo = []</code></p><ul><li>Set/compute <code>topo</code> (store a value in a variable).</li></ul><p><strong>Line 61:</strong> <code>visited = set()</code></p><ul><li>Set/compute <code>visited</code> (store a value in a variable).</li></ul><p><strong>Line 62:</strong> <code>def build_topo(v):</code></p><ul><li>Define a function named <code>build_topo</code> (a reusable block of code).</li></ul><p><strong>Line 63:</strong> <code>if v not in visited:</code></p><ul><li>Check a condition; if it’s true, run the indented block below.</li></ul><p><strong>Line 64:</strong> <code>visited.add(v)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 65:</strong> <code>for child in v._children:</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 66:</strong> <code>build_topo(child)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 67:</strong> <code>topo.append(v)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 68:</strong> <code>build_topo(self)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 69:</strong> <code>self.grad = 1</code></p><ul><li>Set/compute <code>self.grad</code> (store a value in a variable).</li></ul><p><strong>Line 70:</strong> <code>for v in reversed(topo):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 71:</strong> <code>for child, local_grad in zip(v._children, v._local_grads):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 72:</strong> <code>child.grad += local_grad * v.grad</code></p><ul><li>Set/compute <code>child.grad +</code> (store a value in a variable).</li></ul><p><strong>Line 73:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 74:</strong> <code># Initialize the parameters, to store the knowledge of the model.</code></p><ul><li>Initialize the parameters, to store the knowledge of the model.</li></ul><p><strong>Line 75:</strong> <code>n_embd = 16 # embedding dimension</code></p><ul><li>Set/compute <code>n_embd</code> (store a value in a variable).</li></ul><p><strong>Line 76:</strong> <code>n_head = 4 # number of attention heads</code></p><ul><li>Set/compute <code>n_head</code> (store a value in a variable).</li></ul><p><strong>Line 77:</strong> <code>n_layer = 1 # number of layers</code></p><ul><li>Set/compute <code>n_layer</code> (store a value in a variable).</li></ul><p><strong>Line 78:</strong> <code>block_size = 16 # maximum sequence length</code></p><ul><li>Set/compute <code>block_size</code> (store a value in a variable).</li></ul><p><strong>Line 79:</strong> <code>head_dim = n_embd // n_head # dimension of each head</code></p><ul><li>Set/compute <code>head_dim</code> (store a value in a variable).</li></ul><p><strong>Line 80:</strong> <code>matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]</code></p><ul><li>Set/compute <code>matrix</code> (store a value in a variable).</li></ul><p><strong>Line 81:</strong> <code>state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}</code></p><ul><li>Set/compute <code>state_dict</code> (store a value in a variable).</li></ul><p><strong>Line 82:</strong> <code>for i in range(n_layer):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 83:</strong> <code>state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)</code></p><ul><li>Set/compute <code>state_dict[f'layer{i}.attn_wq']</code> (store a value in a variable).</li></ul><p><strong>Line 84:</strong> <code>state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)</code></p><ul><li>Set/compute <code>state_dict[f'layer{i}.attn_wk']</code> (store a value in a variable).</li></ul><p><strong>Line 85:</strong> <code>state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)</code></p><ul><li>Set/compute <code>state_dict[f'layer{i}.attn_wv']</code> (store a value in a variable).</li></ul><p><strong>Line 86:</strong> <code>state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)</code></p><ul><li>Set/compute <code>state_dict[f'layer{i}.attn_wo']</code> (store a value in a variable).</li></ul><p><strong>Line 87:</strong> <code>state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)</code></p><ul><li>Set/compute <code>state_dict[f'layer{i}.mlp_fc1']</code> (store a value in a variable).</li></ul><p><strong>Line 88:</strong> <code>state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)</code></p><ul><li>Set/compute <code>state_dict[f'layer{i}.mlp_fc2']</code> (store a value in a variable).</li></ul><p><strong>Line 89:</strong> <code>params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]</code></p><ul><li>Set/compute <code>params</code> (store a value in a variable).</li></ul><p><strong>Line 90:</strong> <code>print(f"num params: {len(params)}")</code></p><ul><li>Print some information to the console so we can see what’s happening.</li></ul><p><strong>Line 91:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 92:</strong> <code># Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.</code></p><ul><li>Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.</li></ul><p><strong>Line 93:</strong> <code># Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU</code></p><ul><li>Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU</li></ul><p><strong>Line 94:</strong> <code>def linear(x, w):</code></p><ul><li>Define a function named <code>linear</code> (a reusable block of code).</li></ul><p><strong>Line 95:</strong> <code>return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]</code></p><ul><li>Return a value from this function.</li></ul><p><strong>Line 96:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 97:</strong> <code>def softmax(logits):</code></p><ul><li>Define a function named <code>softmax</code> (a reusable block of code).</li></ul><p><strong>Line 98:</strong> <code>max_val = max(val.data for val in logits)</code></p><ul><li>Set/compute <code>max_val</code> (store a value in a variable).</li></ul><p><strong>Line 99:</strong> <code>exps = [(val - max_val).exp() for val in logits]</code></p><ul><li>Set/compute <code>exps</code> (store a value in a variable).</li></ul><p><strong>Line 100:</strong> <code>total = sum(exps)</code></p><ul><li>Set/compute <code>total</code> (store a value in a variable).</li></ul><p><strong>Line 101:</strong> <code>return [e / total for e in exps]</code></p><ul><li>Return a value from this function.</li></ul><p><strong>Line 102:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 103:</strong> <code>def rmsnorm(x):</code></p><ul><li>Define a function named <code>rmsnorm</code> (a reusable block of code).</li></ul><p><strong>Line 104:</strong> <code>ms = sum(xi * xi for xi in x) / len(x)</code></p><ul><li>Set/compute <code>ms</code> (store a value in a variable).</li></ul><p><strong>Line 105:</strong> <code>scale = (ms + 1e-5) ** -0.5</code></p><ul><li>Set/compute <code>scale</code> (store a value in a variable).</li></ul><p><strong>Line 106:</strong> <code>return [xi * scale for xi in x]</code></p><ul><li>Return a value from this function.</li></ul><p><strong>Line 107:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 108:</strong> <code>def gpt(token_id, pos_id, keys, values):</code></p><ul><li>Define a function named <code>gpt</code> (a reusable block of code).</li></ul><p><strong>Line 109:</strong> <code>tok_emb = state_dict['wte'][token_id] # token embedding</code></p><ul><li>Set/compute <code>tok_emb</code> (store a value in a variable).</li></ul><p><strong>Line 110:</strong> <code>pos_emb = state_dict['wpe'][pos_id] # position embedding</code></p><ul><li>Set/compute <code>pos_emb</code> (store a value in a variable).</li></ul><p><strong>Line 111:</strong> <code>x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 112:</strong> <code>x = rmsnorm(x)</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 113:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 114:</strong> <code>for li in range(n_layer):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 115:</strong> <code># 1) Multi-head attention block</code></p><ul><li><ol><li>Multi-head attention block</li></ol></li></ul><p><strong>Line 116:</strong> <code>x_residual = x</code></p><ul><li>Set/compute <code>x_residual</code> (store a value in a variable).</li></ul><p><strong>Line 117:</strong> <code>x = rmsnorm(x)</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 118:</strong> <code>q = linear(x, state_dict[f'layer{li}.attn_wq'])</code></p><ul><li>Set/compute <code>q</code> (store a value in a variable).</li></ul><p><strong>Line 119:</strong> <code>k = linear(x, state_dict[f'layer{li}.attn_wk'])</code></p><ul><li>Set/compute <code>k</code> (store a value in a variable).</li></ul><p><strong>Line 120:</strong> <code>v = linear(x, state_dict[f'layer{li}.attn_wv'])</code></p><ul><li>Set/compute <code>v</code> (store a value in a variable).</li></ul><p><strong>Line 121:</strong> <code>keys[li].append(k)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 122:</strong> <code>values[li].append(v)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 123:</strong> <code>x_attn = []</code></p><ul><li>Set/compute <code>x_attn</code> (store a value in a variable).</li></ul><p><strong>Line 124:</strong> <code>for h in range(n_head):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 125:</strong> <code>hs = h * head_dim</code></p><ul><li>Set/compute <code>hs</code> (store a value in a variable).</li></ul><p><strong>Line 126:</strong> <code>q_h = q[hs:hs+head_dim]</code></p><ul><li>Set/compute <code>q_h</code> (store a value in a variable).</li></ul><p><strong>Line 127:</strong> <code>k_h = [ki[hs:hs+head_dim] for ki in keys[li]]</code></p><ul><li>Set/compute <code>k_h</code> (store a value in a variable).</li></ul><p><strong>Line 128:</strong> <code>v_h = [vi[hs:hs+head_dim] for vi in values[li]]</code></p><ul><li>Set/compute <code>v_h</code> (store a value in a variable).</li></ul><p><strong>Line 129:</strong> <code>attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]</code></p><ul><li>Set/compute <code>attn_logits</code> (store a value in a variable).</li></ul><p><strong>Line 130:</strong> <code>attn_weights = softmax(attn_logits)</code></p><ul><li>Set/compute <code>attn_weights</code> (store a value in a variable).</li></ul><p><strong>Line 131:</strong> <code>head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]</code></p><ul><li>Set/compute <code>head_out</code> (store a value in a variable).</li></ul><p><strong>Line 132:</strong> <code>x_attn.extend(head_out)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 133:</strong> <code>x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 134:</strong> <code>x = [a + b for a, b in zip(x, x_residual)]</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 135:</strong> <code># 2) MLP block</code></p><ul><li><ol start=2><li>MLP block</li></ol></li></ul><p><strong>Line 136:</strong> <code>x_residual = x</code></p><ul><li>Set/compute <code>x_residual</code> (store a value in a variable).</li></ul><p><strong>Line 137:</strong> <code>x = rmsnorm(x)</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 138:</strong> <code>x = linear(x, state_dict[f'layer{li}.mlp_fc1'])</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 139:</strong> <code>x = [xi.relu() for xi in x]</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 140:</strong> <code>x = linear(x, state_dict[f'layer{li}.mlp_fc2'])</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 141:</strong> <code>x = [a + b for a, b in zip(x, x_residual)]</code></p><ul><li>Set/compute <code>x</code> (store a value in a variable).</li></ul><p><strong>Line 142:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 143:</strong> <code>logits = linear(x, state_dict['lm_head'])</code></p><ul><li>Set/compute <code>logits</code> (store a value in a variable).</li></ul><p><strong>Line 144:</strong> <code>return logits</code></p><ul><li>Return a value from this function.</li></ul><p><strong>Line 145:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 146:</strong> <code># Let there be Adam, the blessed optimizer and its buffers</code></p><ul><li>Let there be Adam, the blessed optimizer and its buffers</li></ul><p><strong>Line 147:</strong> <code>learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8</code></p><ul><li>Set/compute <code>learning_rate, beta1, beta2, eps_adam</code> (store a value in a variable).</li></ul><p><strong>Line 148:</strong> <code>m = [0.0] * len(params) # first moment buffer</code></p><ul><li>Set/compute <code>m</code> (store a value in a variable).</li></ul><p><strong>Line 149:</strong> <code>v = [0.0] * len(params) # second moment buffer</code></p><ul><li>Set/compute <code>v</code> (store a value in a variable).</li></ul><p><strong>Line 150:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 151:</strong> <code># Repeat in sequence</code></p><ul><li>Repeat in sequence</li></ul><p><strong>Line 152:</strong> <code>num_steps = 1000 # number of training steps</code></p><ul><li>Set/compute <code>num_steps</code> (store a value in a variable).</li></ul><p><strong>Line 153:</strong> <code>for step in range(num_steps):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 154:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 155:</strong> <code># Take single document, tokenize it, surround it with BOS special token on both sides</code></p><ul><li>Take single document, tokenize it, surround it with BOS special token on both sides</li></ul><p><strong>Line 156:</strong> <code>doc = docs[step % len(docs)]</code></p><ul><li>Set/compute <code>doc</code> (store a value in a variable).</li></ul><p><strong>Line 157:</strong> <code>tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]</code></p><ul><li>Set/compute <code>tokens</code> (store a value in a variable).</li></ul><p><strong>Line 158:</strong> <code>n = min(block_size, len(tokens) - 1)</code></p><ul><li>Set/compute <code>n</code> (store a value in a variable).</li></ul><p><strong>Line 159:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 160:</strong> <code># Forward the token sequence through the model, building up the computation graph all the way to the loss.</code></p><ul><li>Forward the token sequence through the model, building up the computation graph all the way to the loss.</li></ul><p><strong>Line 161:</strong> <code>keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]</code></p><ul><li>Set/compute <code>keys, values</code> (store a value in a variable).</li></ul><p><strong>Line 162:</strong> <code>losses = []</code></p><ul><li>Set/compute <code>losses</code> (store a value in a variable).</li></ul><p><strong>Line 163:</strong> <code>for pos_id in range(n):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 164:</strong> <code>token_id, target_id = tokens[pos_id], tokens[pos_id + 1]</code></p><ul><li>Set/compute <code>token_id, target_id</code> (store a value in a variable).</li></ul><p><strong>Line 165:</strong> <code>logits = gpt(token_id, pos_id, keys, values)</code></p><ul><li>Set/compute <code>logits</code> (store a value in a variable).</li></ul><p><strong>Line 166:</strong> <code>probs = softmax(logits)</code></p><ul><li>Set/compute <code>probs</code> (store a value in a variable).</li></ul><p><strong>Line 167:</strong> <code>loss_t = -probs[target_id].log()</code></p><ul><li>Set/compute <code>loss_t</code> (store a value in a variable).</li></ul><p><strong>Line 168:</strong> <code>losses.append(loss_t)</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 169:</strong> <code>loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.</code></p><ul><li>Set/compute <code>loss</code> (store a value in a variable).</li></ul><p><strong>Line 170:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 171:</strong> <code># Backward the loss, calculating the gradients with respect to all model parameters.</code></p><ul><li>Backward the loss, calculating the gradients with respect to all model parameters.</li></ul><p><strong>Line 172:</strong> <code>loss.backward()</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 173:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 174:</strong> <code># Adam optimizer update: update the model parameters based on the corresponding gradients.</code></p><ul><li>Adam optimizer update: update the model parameters based on the corresponding gradients.</li></ul><p><strong>Line 175:</strong> <code>lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay</code></p><ul><li>Set/compute <code>lr_t</code> (store a value in a variable).</li></ul><p><strong>Line 176:</strong> <code>for i, p in enumerate(params):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 177:</strong> <code>m[i] = beta1 * m[i] + (1 - beta1) * p.grad</code></p><ul><li>Set/compute <code>m[i]</code> (store a value in a variable).</li></ul><p><strong>Line 178:</strong> <code>v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2</code></p><ul><li>Set/compute <code>v[i]</code> (store a value in a variable).</li></ul><p><strong>Line 179:</strong> <code>m_hat = m[i] / (1 - beta1 ** (step + 1))</code></p><ul><li>Set/compute <code>m_hat</code> (store a value in a variable).</li></ul><p><strong>Line 180:</strong> <code>v_hat = v[i] / (1 - beta2 ** (step + 1))</code></p><ul><li>Set/compute <code>v_hat</code> (store a value in a variable).</li></ul><p><strong>Line 181:</strong> <code>p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)</code></p><ul><li>Set/compute <code>p.data -</code> (store a value in a variable).</li></ul><p><strong>Line 182:</strong> <code>p.grad = 0</code></p><ul><li>Set/compute <code>p.grad</code> (store a value in a variable).</li></ul><p><strong>Line 183:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 184:</strong> <code>print(f"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}")</code></p><ul><li>Print some information to the console so we can see what’s happening.</li></ul><p><strong>Line 185:</strong> ``</p><ul><li>(blank line)</li></ul><p><strong>Line 186:</strong> <code># Inference: may the model babble back to us</code></p><ul><li>Inference: may the model babble back to us</li></ul><p><strong>Line 187:</strong> <code>temperature = 0.5 # in (0, 1], control the "creativity" of generated text, low to high</code></p><ul><li>Set/compute <code>temperature</code> (store a value in a variable).</li></ul><p><strong>Line 188:</strong> <code>print("\n--- inference (new, hallucinated names) ---")</code></p><ul><li>Print some information to the console so we can see what’s happening.</li></ul><p><strong>Line 189:</strong> <code>for sample_idx in range(20):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 190:</strong> <code>keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]</code></p><ul><li>Set/compute <code>keys, values</code> (store a value in a variable).</li></ul><p><strong>Line 191:</strong> <code>token_id = BOS</code></p><ul><li>Set/compute <code>token_id</code> (store a value in a variable).</li></ul><p><strong>Line 192:</strong> <code>sample = []</code></p><ul><li>Set/compute <code>sample</code> (store a value in a variable).</li></ul><p><strong>Line 193:</strong> <code>for pos_id in range(block_size):</code></p><ul><li>Start a loop: repeat the next indented block for each item in a collection.</li></ul><p><strong>Line 194:</strong> <code>logits = gpt(token_id, pos_id, keys, values)</code></p><ul><li>Set/compute <code>logits</code> (store a value in a variable).</li></ul><p><strong>Line 195:</strong> <code>probs = softmax([l / temperature for l in logits])</code></p><ul><li>Set/compute <code>probs</code> (store a value in a variable).</li></ul><p><strong>Line 196:</strong> <code>token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]</code></p><ul><li>Set/compute <code>token_id</code> (store a value in a variable).</li></ul><p><strong>Line 197:</strong> <code>if token_id == BOS:</code></p><ul><li>Check a condition; if it’s true, run the indented block below.</li></ul><p><strong>Line 198:</strong> <code>break</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 199:</strong> <code>sample.append(uchars[token_id])</code></p><ul><li>Do this step as written (part of the algorithm’s mechanics).</li></ul><p><strong>Line 200:</strong> <code>print(f"sample {sample_idx+1:2d}: {''.join(sample)}")</code></p><ul><li>Print some information to the console so we can see what’s happening.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://emino.app/tags/gpt/>GPT</a></li><li><a href=https://emino.app/tags/karpathy/>Karpathy</a></li><li><a href=https://emino.app/tags/python/>Python</a></li><li><a href=https://emino.app/tags/explained/>Explained</a></li></ul><nav class=paginav><a class=prev href=https://emino.app/posts/microgpt-karpathy-part-2-shapes-and-a-tiny-forward-pass/><span class=title>« Prev</span><br><span>microgpt.py (Karpathy) — Part 2: shapes at every step + a tiny forward pass</span>
</a><a class=next href=https://emino.app/posts/microgpt-karpathy-line-by-line/><span class=title>Next »</span><br><span>microgpt.py (Karpathy) — the code in plain English, line by line</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on x" href="https://x.com/intent/tweet/?text=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29&amp;url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f&amp;hashtags=GPT%2cKarpathy%2cPython%2cExplained"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f&amp;title=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29&amp;summary=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29&amp;source=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f&title=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on whatsapp" href="https://api.whatsapp.com/send?text=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29%20-%20https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on telegram" href="https://telegram.me/share/url?text=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29&amp;url=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share microgpt.py explained in plain English (line by line) on ycombinator" href="https://news.ycombinator.com/submitlink?t=microgpt.py%20explained%20in%20plain%20English%20%28line%20by%20line%29&u=https%3a%2f%2femino.app%2fposts%2fmicrogpt-line-by-line-plain-english%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://emino.app/>Art & Articles by Emin Mahrt</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>